{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/04-chat-memory.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEZdSobItaTI"
      },
      "source": [
        "### LangChain Essentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaGqZi42taTJ"
      },
      "source": [
        "# Conversational Memory for OpenAI - LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkg11xv9taTK"
      },
      "source": [
        "Conversational memory allows our chatbots and agents to remember previous interactions within a conversation. Without conversational memory, our chatbots would only ever be able to respond to the last message they received, essentially forgetting all previous messages with each new message.\n",
        "\n",
        "Naturally, conversations require our chatbots to be able to respond over multiple interactions and refer to previous messages to understand the context of the conversation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euKN4BNCtaTK"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ If using LangSmith, add your API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSfe1JIwtaTK",
        "outputId": "8afe3d02-2f49-4dac-d3e8-b8968f690074"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"aurelioai-langchain-course-chat-memory-openai\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ57c5BxtaTL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAYjdtcPtaTL"
      },
      "source": [
        "## LangChain's Memory Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czt55jO1taTL"
      },
      "source": [
        "LangChain versions `0.0.x` consisted of various conversational memory types. Most of these are due for deprecation but still hold value in understanding the different approaches that we can take to building conversational memory.\n",
        "\n",
        "Throughout the notebook we will be referring to these _older_ memory types and then rewriting them using the recommended `RunnableWithMessageHistory` class. We will learn about:\n",
        "\n",
        "* `ConversationBufferMemory`: the simplest and most intuitive form of conversational memory, keeping track of a conversation without any additional bells and whistles.\n",
        "* `ConversationBufferWindowMemory`: similar to `ConversationBufferMemory`, but only keeps track of the last `k` messages.\n",
        "* `ConversationSummaryMemory`: rather than keeping track of the entire conversation, this memory type keeps track of a summary of the conversation.\n",
        "* `ConversationSummaryBufferMemory`: merges the `ConversationSummaryMemory` and `ConversationTokenBufferMemory` types.\n",
        "\n",
        "We'll work through each of these memory types in turn, and rewrite each one using the `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brmml7G-taTL"
      },
      "source": [
        "## Initialize our LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1vhQcsxtaTL"
      },
      "source": [
        "Before jumping into our memory types, let's initialize our LLM. We will use OpenAI's `gpt-4o-mini` model, if you need an API key you can get one from [OpenAI's website](https://platform.openai.com/settings/organization/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNMunYQZtaTL",
        "outputId": "0d8735a5-68fb-476b-9ca9-201f8a4104bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\micro\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "\n",
        "# For normal accurate responses\n",
        "llm = ChatGoogleGenerativeAI(temperature=0.0, model=\"gemini-2.5-flash\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsRVD0HctaTM"
      },
      "source": [
        "## 1. `ConversationBufferMemory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40zJzXXXtaTM"
      },
      "source": [
        "`ConversationBufferMemory` is the simplest form of conversational memory, it is _literally_ just a place that we store messages, and then use to feed messages into our LLM.\n",
        "\n",
        "Let's start with LangChain's original `ConversationBufferMemory` object, we are setting `return_messages=True` to return the messages as a list of `ChatMessage` objects — unless using a non-chat model we would always set this to `True` as without it the messages are passed as a direct string which can lead to unexpected behavior from chat LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tY-BxLW3taTM",
        "outputId": "96f88376-cf9a-45f3-bde1-90fbf896022e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\micro\\AppData\\Local\\Temp\\ipykernel_1996\\1448044083.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(return_messages=True)\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMkCBXxitaTM"
      },
      "source": [
        "There are several ways that we can add messages to our memory, using the `save_context` method we can add a user query (via the `input` key) and the AI's response (via the `output` key). So, to create the following conversation:\n",
        "\n",
        "```\n",
        "User: Hi, my name is James\n",
        "AI: Hey James, what's up? I'm an AI model called Zeta.\n",
        "User: I'm researching the different types of conversational memory.\n",
        "AI: That's interesting, what are some examples?\n",
        "User: I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
        "AI: That's interesting, what's the difference?\n",
        "User: Buffer memory just stores the entire conversation, right?\n",
        "AI: That makes sense, what about ConversationBufferWindowMemory?\n",
        "User: Buffer window memory stores the last k messages, dropping the rest.\n",
        "AI: Very cool!\n",
        "```\n",
        "\n",
        "We do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mUPUE9DhtaTM"
      },
      "outputs": [],
      "source": [
        "memory.save_context(\n",
        "    {\"input\": \"Hi, my name is James\"},  # user message\n",
        "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"I'm researching the different types of conversational memory.\"},  # user message\n",
        "    {\"output\": \"That's interesting, what are some examples?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"},  # user message\n",
        "    {\"output\": \"That's interesting, what's the difference?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"Buffer memory just stores the entire conversation, right?\"},  # user message\n",
        "    {\"output\": \"That makes sense, what about ConversationBufferWindowMemory?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"},  # user message\n",
        "    {\"output\": \"Very cool!\"}  # AI response\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw6C4W54taTM"
      },
      "source": [
        "Before using the memory, we need to load in any variables for that memory type — in this case, there are none, so we just pass an empty dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRLwtZvWtaTM",
        "outputId": "c7781545-b5c9-4d5e-cb26-745dc3264c84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_qjbVustaTM"
      },
      "source": [
        "With that, we've created our buffer memory. Before feeding it into our LLM let's quickly view the alternative method for adding messages to our memory. With this other method, we pass individual user and AI messages via the `add_user_message` and `add_ai_message` methods. To reproduce what we did above, we do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOjy4g_GtaTM",
        "outputId": "4447488e-aa8c-4b5f-c2f6-11744453f777"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
        "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
        "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
        "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1tkc6DOtaTM"
      },
      "source": [
        "The outcome is exactly the same in either case. To pass this onto our LLM, we need to create a `ConversationChain` object — which is already deprecated in favor of the `RunnableWithMessageHistory` class, which we will cover in a moment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwgjzC0ktaTM",
        "outputId": "231215d1-568b-4546-daa1-f12da4e47475"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\micro\\AppData\\Local\\Temp\\ipykernel_1996\\839683240.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  chain = ConversationChain(\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx1fMZLWtaTN",
        "outputId": "6e185883-44a8-4538-e068-be8f54183d00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
            "Human: what is my name again?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'what is my name again?',\n",
              " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Your name is James! You told me right at the beginning of our chat, \"Hi, my name is James.\" It\\'s good to keep track of who I\\'m talking to! Is there anything else I can help you remember from our conversation, or perhaps something new you\\'d like to discuss about conversational memory? I\\'m all ears!', additional_kwargs={}, response_metadata={})],\n",
              " 'response': 'Your name is James! You told me right at the beginning of our chat, \"Hi, my name is James.\" It\\'s good to keep track of who I\\'m talking to! Is there anything else I can help you remember from our conversation, or perhaps something new you\\'d like to discuss about conversational memory? I\\'m all ears!'}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"what is my name again?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQTzPaovtaTN"
      },
      "source": [
        "### `ConversationBufferMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLZa82dhtaTN"
      },
      "source": [
        "As mentioned, the `ConversationBufferMemory` type is due for deprecation. Instead, we can use the `RunnableWithMessageHistory` class to implement the same functionality.\n",
        "\n",
        "When implementing `RunnableWithMessageHistory` we will use **L**ang**C**hain **E**xpression **L**anguage (LCEL) and for this we need to define our prompt template and LLM components. Our `llm` has already been defined, so now we just define a `ChatPromptTemplate` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UijZzCtutaTN"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    ChatPromptTemplate\n",
        ")\n",
        "\n",
        "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnEAavfntaTN"
      },
      "source": [
        "We can link our `prompt_template` and our `llm` together to create a pipeline via LCEL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ywxify04taTN"
      },
      "outputs": [],
      "source": [
        "pipeline = prompt_template | llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jhOF-uVtaTN"
      },
      "source": [
        "Our `RunnableWithMessageHistory` requires our `pipeline` to be wrapped in a `RunnableWithMessageHistory` object. This object requires a few input parameters. One of those is `get_session_history`, which requires a function that returns a `ChatMessageHistory` object based on a session ID. We define this function ourselves:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BXKmcliGtaTN"
      },
      "outputs": [],
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "\n",
        "chat_map = {}\n",
        "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39LBschEtaTN"
      },
      "source": [
        "We also need to tell our runnable which variable name to use for the chat history (ie `history`) and which to use for the user's query (ie `query`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zHQRgNfrtaTO"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wwEeNAVtaTO"
      },
      "source": [
        "Now we invoke our runnable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ESDoue1taTO",
        "outputId": "7f5606e2-dee4-4b59-e828-e8cca77b0bad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--5486918c-cef7-484b-ac67-c7eaa7a741e7-0', usage_metadata={'input_tokens': 15, 'output_tokens': 18, 'total_tokens': 62, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avL-i76_taTO"
      },
      "source": [
        "Our chat history will now be memorized and retrieved whenever we invoke our runnable with the same session ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgfzWIBstaTO",
        "outputId": "960b9d46-f422-4486-d36b-e45623394c1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--5b0eaa96-ea53-428b-a5cc-2189e97e1a22-0', usage_metadata={'input_tokens': 41, 'output_tokens': 5, 'total_tokens': 71, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"What is my name again?\"},\n",
        "    config={\"session_id\": \"id_123\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmjSuam-taTO"
      },
      "source": [
        "We have now recreated the `ConversationBufferMemory` type using the `RunnableWithMessageHistory` class. Let's continue onto other memory types and see how these can be implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0MliXn-taTO"
      },
      "source": [
        "## 2. `ConversationBufferWindowMemory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lziswUkqtaTO"
      },
      "source": [
        "The `ConversationBufferWindowMemory` type is similar to `ConversationBufferMemory`, but only keeps track of the last `k` messages. There are a few reasons why we would want to keep only the last `k` messages:\n",
        "\n",
        "* More messages mean more tokens are sent with each request, more tokens increases latency _and_ cost.\n",
        "\n",
        "* LLMs tend to perform worse when given more tokens, making them more likely to deviate from instructions, hallucinate, or _\"forget\"_ information provided to them. Conciseness is key to high performing LLMs.\n",
        "\n",
        "* If we keep _all_ messages we will eventually hit the LLM's context window limit, by adding a window size `k` we can ensure we never hit this limit.\n",
        "\n",
        "The buffer window solves many problems that we encounter with the standard buffer memory, while still being a very simple and intuitive form of conversational memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBb46qDDtaTO",
        "outputId": "99e71fe7-48be-4579-d404-e5ec8c933e09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\micro\\AppData\\Local\\Temp\\ipykernel_1996\\3216785012.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=4, return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to6rLd_ataTO"
      },
      "source": [
        "We populate this memory using the same methods as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pfxuLS7taTP",
        "outputId": "51772b66-bda9-406a-b6e3-d026c231f24f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
        "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
        "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
        "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV_6vOOwtaTP"
      },
      "source": [
        "As before, we use the `ConversationChain` object (again, this is deprecated and we will rewrite it with `RunnableWithMessageHistory` in a moment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "opmjk94ltaTP"
      },
      "outputs": [],
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ-g586vtaTP"
      },
      "source": [
        "Now let's see if our LLM remembers our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWa5mhzftaTP",
        "outputId": "19a63db6-82ff-4ba3-84de-e3d4c42f3ce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
            "Human: what is my name again?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'what is my name again?',\n",
              " 'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})],\n",
              " 'response': \"Oh dear, I'm so sorry, but I don't actually know your name! You haven't told it to me yet in our conversation. I'm just an AI, and I only remember what we've talked about in this specific chat.\\n\\nWe've been discussing conversational memory types like `ConversationBufferMemory` and `ConversationBufferWindowMemory`, and how one stores the whole chat while the other keeps only the last `k` messages. But your name hasn't come up!\\n\\nWould you like to tell me your name? I'd be happy to remember it for the rest of our chat!\"}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"what is my name again?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GrFsMbStaTP"
      },
      "source": [
        "The reason our LLM can no longer remember our name is because we have set the `k` parameter to `4`, meaning that only the last messages are stored in memory, as we can see above this does not include the first message where we introduced ourselves.\n",
        "\n",
        "Based on the agent forgetting our name, we might wonder _why_ we would ever use this memory type compared to the standard buffer memory. Well, as with most things in AI, it is always a trade-off. Here we are able to support much longer conversations, use less tokens, and improve latency — but these come at the cost of forgetting non-recent messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alyXf5nKtaTP"
      },
      "source": [
        "### `ConversationBufferWindowMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxaUDp7GtaTP"
      },
      "source": [
        "To implement this memory type using the `RunnableWithMessageHistory` class, we can use the same approach as before. We define our `prompt_template` and `llm` as before, and then wrap our pipeline in a `RunnableWithMessageHistory` object.\n",
        "\n",
        "For the window feature, we need to define a custom version of the `InMemoryChatMessageHistory` class that removes any messages beyond the last `k` messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fQUv8yPltaTP"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, k: int):\n",
        "        super().__init__(k=k)\n",
        "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages.\n",
        "        \"\"\"\n",
        "        self.messages.extend(messages)\n",
        "        self.messages = self.messages[-self.k:]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uoDJ38vztaTP"
      },
      "outputs": [],
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
        "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
        "    # remove anything beyond the last\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "GsXhm811taTP"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the history\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQR8TZLGtaTQ"
      },
      "source": [
        "Now we invoke our runnable, this time passing a `k` parameter via the `config` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBCnrghLtaTQ",
        "outputId": "05455096-12c0-40eb-8b62-eaaddeb5204d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_chat_history called with session_id=id_k4 and k=4\n",
            "Initializing BufferWindowMessageHistory with k=4\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--98203efd-a30c-4bc4-8b18-0771dc21a843-0', usage_metadata={'input_tokens': 15, 'output_tokens': 18, 'total_tokens': 62, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AvqsaA3taTQ"
      },
      "source": [
        "We can also modify the messages that are stored in memory by modifying the records inside the `chat_map` dictionary directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDMrCS26taTQ",
        "outputId": "116bcb51-3585-404c-9e4a-0f5391b0136f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_map[\"id_k4\"].clear()  # clear the history\n",
        "\n",
        "# manually insert history\n",
        "chat_map[\"id_k4\"].add_user_message(\"Hi, my name is James\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"I'm an AI model called Zeta.\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"Very cool!\")\n",
        "\n",
        "chat_map[\"id_k4\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bzArAVDtaTQ"
      },
      "source": [
        "Now let's see at which `k` value our LLM remembers our name — from the above we can already see that with `k=4` our name is not mentioned, so when running with `k=4` we should expect the LLM to forget our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yn3K6fxtaTQ",
        "outputId": "de1b7db8-214e-4936-d322-635b19571ad1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_chat_history called with session_id=id_k4 and k=4\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"I don't know your name. I don't have access to personal information like that.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--af3f37a5-3ab3-4a0f-bc15-27455f090b55-0', usage_metadata={'input_tokens': 56, 'output_tokens': 20, 'total_tokens': 111, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"what is my name again?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN7am67YtaTQ"
      },
      "source": [
        "Now let's initialize a new session with `k=14`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOXU6nI0taTQ",
        "outputId": "77812ee2-d549-4321-b632-a268d5ad239c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_chat_history called with session_id=id_k14 and k=14\n",
            "Initializing BufferWindowMessageHistory with k=14\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hi James! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--6de0e79c-3bb6-4be4-bd48-23fda4c1991b-0', usage_metadata={'input_tokens': 15, 'output_tokens': 18, 'total_tokens': 67, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGPyLeEOtaTQ"
      },
      "source": [
        "We'll manually insert the remaining messages as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuyHMnkCtaTQ",
        "outputId": "bc214490-f26e-4ee9-9763-2ff107743ba2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Hi James! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--6de0e79c-3bb6-4be4-bd48-23fda4c1991b-0', usage_metadata={'input_tokens': 15, 'output_tokens': 18, 'total_tokens': 67, 'input_token_details': {'cache_read': 0}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_map[\"id_k14\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"Very cool!\")\n",
        "\n",
        "chat_map[\"id_k14\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RVgWS6EtaTQ"
      },
      "source": [
        "Now let's see if the LLM remembers our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8X1xV9ItaTQ",
        "outputId": "45b6425c-8d94-4900-9c9d-4fb6db4583a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_chat_history called with session_id=id_k14 and k=14\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--440078e1-e9a1-453e-9d40-140474f08d84-0', usage_metadata={'input_tokens': 133, 'output_tokens': 5, 'total_tokens': 163, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"what is my name again?\"},\n",
        "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3SqKjEvtaTQ",
        "outputId": "4ada095e-c9f5-4504-ceb6-cf7a40db677d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Hi James! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--6de0e79c-3bb6-4be4-bd48-23fda4c1991b-0', usage_metadata={'input_tokens': 15, 'output_tokens': 18, 'total_tokens': 67, 'input_token_details': {'cache_read': 0}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--440078e1-e9a1-453e-9d40-140474f08d84-0', usage_metadata={'input_tokens': 133, 'output_tokens': 5, 'total_tokens': 163, 'input_token_details': {'cache_read': 0}})]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_map[\"id_k14\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R91uVCqFtaTQ"
      },
      "source": [
        "That's it! We've rewritten our buffer window memory using the recommended `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK51EzantaTR"
      },
      "source": [
        "## 3. `ConversationSummaryMemory`\n",
        "\n",
        "Next up we have `ConversationSummaryMemory`, this memory type keeps track of a summary of the conversation rather than the entire conversation. This is useful for long conversations where we don't need to keep track of the entire conversation, but we do want to keep some thread of the full conversation.\n",
        "\n",
        "As before, we'll start with the original memory class before reimplementing it with the `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1DRv1umtaTR",
        "outputId": "6d6f93b9-31fd-4313-8942-aa5d39546831"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\micro\\AppData\\Local\\Temp\\ipykernel_1996\\988334424.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(llm=llm)\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "memory = ConversationSummaryMemory(llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDK1xTiQtaTR"
      },
      "source": [
        "Unlike with the previous memory types, we need to provide an `llm` to initialize `ConversationSummaryMemory`. The reason for this is that we need an LLM to generate the conversation summaries.\n",
        "\n",
        "Beyond this small tweak, using `ConversationSummaryMemory` is the same as with our previous memory types when using the deprecated `ConversationChain` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "A5hxfssAtaTR"
      },
      "outputs": [],
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5nHb-SqtaTR"
      },
      "source": [
        "Let's test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwCtQA0RtaTR",
        "outputId": "0fe06aa4-e2dc-4b10-d994-296f8b642323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: hello there my name is James\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "New summary:\n",
            "James introduces himself to the AI. The AI introduces itself as a Google-trained large language model, expresses pleasure in meeting James, and asks what brings him to the conversation.\n",
            "Human: I am researching the different types of conversational memory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "James introduces himself to the AI. The AI introduces itself as a Google-trained large language model, expresses pleasure in meeting James, and asks what brings him to the conversation. James states he is researching the different types of conversational memory. The AI finds this fascinating and explains that conversational memory involves an interplay of various memory systems, detailing eight key types: episodic (specific events/conversations), semantic (general knowledge/understanding), working (in-the-moment processing), procedural (social rules/how-to), autobiographical (personal life story), prospective (remembering future actions), source (remembering *where* information came from), and content (remembering *what* was said). The AI concludes by noting these types interact and asks James if he wants to explore any specific type further.\n",
            "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "James introduces himself to the AI. The AI introduces itself as a Google-trained large language model, expresses pleasure in meeting James, and asks what brings him to the conversation. James states he is researching the different types of conversational memory. The AI finds this fascinating and explains that conversational memory involves an interplay of various memory systems, detailing eight key types: episodic (specific events/conversations), semantic (general knowledge/understanding), working (in-the-moment processing), procedural (social rules/how-to), autobiographical (personal life story), prospective (remembering future actions), source (remembering *where* information came from), and content (remembering *what* was said). The AI concludes by noting these types interact and asks James if he wants to explore any specific type further. James then specifies his interest in `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI acknowledges these as practical implementations related to episodic and working memory, and proceeds to explain them in detail. `ConversationBufferMemory` acts like a complete, unedited transcript, storing all messages for full context but suffering from scalability issues, potential cost, and irrelevant information as conversations grow. In contrast, `ConversationBufferWindowMemory` functions as a sliding window, retaining only the `k` most recent messages to manage the LLM's context window, improve efficiency, and maintain focus, though it risks losing older context. The AI summarizes the key differences between the two, highlighting their distinct approaches to history storage, size, context access, LLM context fit, cost/speed, and use cases, noting they are fundamental building blocks for more advanced memory systems. The AI then asks James if this breakdown clarifies their differences and suitability for specific applications.\n",
            "Human: Buffer memory just stores the entire conversation\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "James introduces himself to the AI. The AI introduces itself as a Google-trained large language model, expresses pleasure in meeting James, and asks what brings him to the conversation. James states he is researching the different types of conversational memory. The AI finds this fascinating and explains that conversational memory involves an interplay of various memory systems, detailing eight key types: episodic (specific events/conversations), semantic (general knowledge/understanding), working (in-the-moment processing), procedural (social rules/how-to), autobiographical (personal life story), prospective (remembering future actions), source (remembering *where* information came from), and content (remembering *what* was said). The AI concludes by noting these types interact and asks James if he wants to explore any specific type further. James then specifies his interest in `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI acknowledges these as practical implementations related to episodic and working memory, and proceeds to explain them in detail. `ConversationBufferMemory` acts like a complete, unedited transcript, storing all messages for full context but suffering from scalability issues, potential cost, and irrelevant information as conversations grow. In contrast, `ConversationBufferWindowMemory` functions as a sliding window, retaining only the `k` most recent messages to manage the LLM's context window, improve efficiency, and maintain focus, though it risks losing older context. The AI summarizes the key differences between the two, highlighting their distinct approaches to history storage, size, context access, LLM context fit, cost/speed, and use cases, noting they are fundamental building blocks for more advanced memory systems. The AI then asks James if this breakdown clarifies their differences and suitability for specific applications. James confirms his understanding that `ConversationBufferMemory` stores the entire conversation, which the AI affirms, elaborating that it acts as a full, unedited transcript providing complete context due to its simplicity, while reiterating the challenges this poses for longer interactions.\n",
            "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Buffer window memory stores the last k messages, dropping the rest.',\n",
              " 'history': \"James introduces himself to the AI. The AI introduces itself as a Google-trained large language model, expresses pleasure in meeting James, and asks what brings him to the conversation. James states he is researching the different types of conversational memory. The AI finds this fascinating and explains that conversational memory involves an interplay of various memory systems, detailing eight key types: episodic (specific events/conversations), semantic (general knowledge/understanding), working (in-the-moment processing), procedural (social rules/how-to), autobiographical (personal life story), prospective (remembering future actions), source (remembering *where* information came from), and content (remembering *what* was said). The AI concludes by noting these types interact and asks James if he wants to explore any specific type further. James then specifies his interest in `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI acknowledges these as practical implementations related to episodic and working memory, and proceeds to explain them in detail. `ConversationBufferMemory` acts like a complete, unedited transcript, storing all messages for full context but suffering from scalability issues, potential cost, and irrelevant information as conversations grow. In contrast, `ConversationBufferWindowMemory` functions as a sliding window, retaining only the `k` most recent messages to manage the LLM's context window, improve efficiency, and maintain focus, though it risks losing older context. The AI summarizes the key differences between the two, highlighting their distinct approaches to history storage, size, context access, LLM context fit, cost/speed, and use cases, noting they are fundamental building blocks for more advanced memory systems. The AI then asks James if this breakdown clarifies their differences and suitability for specific applications. James confirms his understanding that `ConversationBufferMemory` stores the entire conversation, which the AI affirms, elaborating that it acts as a full, unedited transcript providing complete context due to its simplicity, while reiterating the challenges this poses for longer interactions.\",\n",
              " 'response': 'That\\'s absolutely spot on, James! You\\'ve got it perfectly. `ConversationBufferWindowMemory` does indeed store only the last `k` messages, and it actively drops anything older than that.\\n\\nThink of it like this: imagine you have a whiteboard, but it\\'s only big enough to hold `k` lines of text. As new messages come in, you write them on the whiteboard. But once the whiteboard is full, to make space for a new message, you have to erase the very oldest message at the top. This \"sliding window\" approach is incredibly useful because it directly addresses the limitations of Large Language Models (LLMs) regarding their context window size.\\n\\nBy keeping only the most recent `k` messages, `ConversationBufferWindowMemory` ensures that:\\n\\n1.  **The input to the LLM remains manageable:** It prevents the conversation history from growing indefinitely, which would eventually exceed the LLM\\'s maximum token limit.\\n2.  **Efficiency is maintained:** Shorter inputs mean faster processing and often lower computational costs.\\n3.  **Focus is preserved:** The LLM is always working with the most recent and usually most relevant parts of the conversation, which can be beneficial for maintaining coherence in ongoing dialogue.\\n\\nOf course, as we touched on, the trade-off is that you do lose the older context. If something important was said way back at the beginning of a long conversation, and it\\'s now outside that `k`-message window, the LLM won\\'t \"remember\" it directly through this memory system. But for many applications where recent context is paramount, it\\'s an excellent and very practical solution!\\n\\nDoes that further clarify how `ConversationBufferWindowMemory` operates and its primary purpose?'}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"hello there my name is James\"})\n",
        "chain.invoke({\"input\": \"I am researching the different types of conversational memory.\"})\n",
        "chain.invoke({\"input\": \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"})\n",
        "chain.invoke({\"input\": \"Buffer memory just stores the entire conversation\"})\n",
        "chain.invoke({\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfgmdXd2taTR"
      },
      "source": [
        "We can see how the conversation summary varies with each new message. Let's see if the LLM is able to recall our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHnh4Y1OtaTR",
        "outputId": "deb8f44e-d100-4a6f-81e7-862608bbe3b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "James introduces himself to the AI. The AI introduces itself as a Google-trained large language model, expresses pleasure in meeting James, and asks what brings him to the conversation. James states he is researching the different types of conversational memory. The AI finds this fascinating and explains that conversational memory involves an interplay of various memory systems, detailing eight key types: episodic (specific events/conversations), semantic (general knowledge/understanding), working (in-the-moment processing), procedural (social rules/how-to), autobiographical (personal life story), prospective (remembering future actions), source (remembering *where* information came from), and content (remembering *what* was said). The AI concludes by noting these types interact and asks James if he wants to explore any specific type further. James then specifies his interest in `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI acknowledges these as practical implementations related to episodic and working memory, and proceeds to explain them in detail. `ConversationBufferMemory` acts like a complete, unedited transcript, storing all messages for full context but suffering from scalability issues, potential cost, and irrelevant information as conversations grow. In contrast, `ConversationBufferWindowMemory` functions as a sliding window, retaining only the `k` most recent messages to manage the LLM's context window, improve efficiency, and maintain focus, though it risks losing older context. The AI summarizes the key differences between the two, highlighting their distinct approaches to history storage, size, context access, LLM context fit, cost/speed, and use cases, noting they are fundamental building blocks for more advanced memory systems. The AI then asks James if this breakdown clarifies their differences and suitability for specific applications. James confirms his understanding that `ConversationBufferMemory` stores the entire conversation, which the AI affirms, elaborating that it acts as a full, unedited transcript providing complete context due to its simplicity, while reiterating the challenges this poses for longer interactions. **James then states that `ConversationBufferWindowMemory` stores the last `k` messages and drops the rest, which the AI confirms as perfectly correct.** The AI further clarifies `ConversationBufferWindowMemory`'s operation using a whiteboard analogy, explaining that it actively drops older messages to make space for new ones. **It highlights the utility of this \"sliding window\" approach for LLMs by ensuring manageable input, maintaining efficiency (faster processing, lower costs), and preserving focus on the most recent and relevant parts of the conversation, while acknowledging the trade-off of losing older context.** The AI then asks if this further clarifies its purpose.\n",
            "Human: What is my name again?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What is my name again?',\n",
              " 'history': 'James introduces himself to the AI. The AI introduces itself as a Google-trained large language model, expresses pleasure in meeting James, and asks what brings him to the conversation. James states he is researching the different types of conversational memory. The AI finds this fascinating and explains that conversational memory involves an interplay of various memory systems, detailing eight key types: episodic (specific events/conversations), semantic (general knowledge/understanding), working (in-the-moment processing), procedural (social rules/how-to), autobiographical (personal life story), prospective (remembering future actions), source (remembering *where* information came from), and content (remembering *what* was said). The AI concludes by noting these types interact and asks James if he wants to explore any specific type further. James then specifies his interest in `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI acknowledges these as practical implementations related to episodic and working memory, and proceeds to explain them in detail. `ConversationBufferMemory` acts like a complete, unedited transcript, storing all messages for full context but suffering from scalability issues, potential cost, and irrelevant information as conversations grow. In contrast, `ConversationBufferWindowMemory` functions as a sliding window, retaining only the `k` most recent messages to manage the LLM\\'s context window, improve efficiency, and maintain focus, though it risks losing older context. The AI summarizes the key differences between the two, highlighting their distinct approaches to history storage, size, context access, LLM context fit, cost/speed, and use cases, noting they are fundamental building blocks for more advanced memory systems. The AI then asks James if this breakdown clarifies their differences and suitability for specific applications. James confirms his understanding that `ConversationBufferMemory` stores the entire conversation, which the AI affirms, elaborating that it acts as a full, unedited transcript providing complete context due to its simplicity, while reiterating the challenges this poses for longer interactions. **James then states that `ConversationBufferWindowMemory` stores the last `k` messages and drops the rest, which the AI confirms as perfectly correct.** The AI further clarifies `ConversationBufferWindowMemory`\\'s operation using a whiteboard analogy, explaining that it actively drops older messages to make space for new ones. **It highlights the utility of this \"sliding window\" approach for LLMs by ensuring manageable input, maintaining efficiency (faster processing, lower costs), and preserving focus on the most recent and relevant parts of the conversation, while acknowledging the trade-off of losing older context.** The AI then asks if this further clarifies its purpose.',\n",
              " 'response': \"Ah, you're James! It's a pleasure to continue our conversation, James. We were just discussing the nuances of `ConversationBufferWindowMemory` and how it manages conversation history. Is there anything else I can clarify about it, or perhaps another type of memory you'd like to delve into?\"}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"What is my name again?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZpfPyEztaTR"
      },
      "source": [
        "As this information was stored in the summary the LLM successfully recalled our name. This may not always be the case, by summarizing the conversation we inevitably compress the full amount of information and so we may lose key details occasionally. Nonetheless, this is a great memory type for long conversations while retaining some key information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snlKvJDUtaTR"
      },
      "source": [
        "### `ConversationSummaryMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9jdJCkztaTR"
      },
      "source": [
        "Let's implement this memory type using the `RunnableWithMessageHistory` class. As with the window buffer memory, we need to define a custom implementation of the `InMemoryChatMessageHistory` class. We'll call this one `ConversationSummaryMessageHistory`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "C09U1WkqtaTR"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "\n",
        "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatGoogleGenerativeAI = Field(default_factory=ChatGoogleGenerativeAI)\n",
        "\n",
        "    def __init__(self, llm: ChatGoogleGenerativeAI):\n",
        "        super().__init__(llm=llm)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages.\n",
        "        \"\"\"\n",
        "        self.messages.extend(messages)\n",
        "        # construct the summary chat messages\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{messages}\"\n",
        "            )\n",
        "        ])\n",
        "        # format the messages and invoke the LLM\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=self.messages.content,\n",
        "                messages=[x.content for x in messages]\n",
        "            )\n",
        "        )\n",
        "        # replace the existing history with a single system summary message\n",
        "        self.messages = [SystemMessage(content=new_summary.content)]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "762Vg5u4taTR"
      },
      "outputs": [],
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, llm: ChatGoogleGenerativeAI) -> ConversationSummaryMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
        "    # return the chat history\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "om78sKCGtaTS"
      },
      "outputs": [],
      "source": [
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatGoogleGenerativeAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPfut3S5taTS"
      },
      "source": [
        "Now we invoke our runnable, this time passing a `llm` parameter via the `config` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1F1m9n2taTS",
        "outputId": "ff68d98f-f5a0-40f9-e603-38450b2cff23"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--0d798160-36db-4e26-8c7c-4d57b1992a16-0', usage_metadata={'input_tokens': 15, 'output_tokens': 18, 'total_tokens': 62, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYqOCHCTtaTS"
      },
      "source": [
        "Let's see what summary was generated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBDeudJftaTS",
        "outputId": "9e2fe892-1f96-4691-e47a-c0e198913e41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--0d798160-36db-4e26-8c7c-4d57b1992a16-0', usage_metadata={'input_tokens': 15, 'output_tokens': 18, 'total_tokens': 62, 'input_token_details': {'cache_read': 0}})]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6uLeGXztaTS"
      },
      "source": [
        "Let's continue the conversation and see if the summary is updated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di7WVWmCtaTS",
        "outputId": "df55157b-d642-41b5-b25d-d0b1be47ba75"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--0d798160-36db-4e26-8c7c-4d57b1992a16-0', usage_metadata={'input_tokens': 15, 'output_tokens': 18, 'total_tokens': 62, 'input_token_details': {'cache_read': 0}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That\\'s a fascinating and crucial area in AI and natural language processing! Conversational memory is what allows a chatbot or AI system to remember past interactions within a conversation, making the dialogue feel more natural, coherent, and personalized. Without it, every turn would be like starting a brand new conversation.\\n\\nThere are several ways to categorize and implement conversational memory, but generally, we can think of them in terms of **short-term** and **long-term** memory, often with different mechanisms for each.\\n\\nHere\\'s a breakdown of the different types:\\n\\n### 1. Short-Term Conversational Memory (Context Window)\\n\\nThis is the most common and fundamental type, primarily used to maintain coherence within a single, ongoing conversation session.\\n\\n*   **Mechanism:** For most modern large language models (LLMs), this is handled by the **context window**. The model is given a fixed number of previous turns (or tokens) from the conversation as input along with the current user query.\\n*   **How it works:** The LLM \"sees\" the recent history of the conversation and uses that information to generate a relevant and contextually appropriate response.\\n*   **Limitations:**\\n    *   **Token Limit:** LLMs have a finite context window size (e.g., 4k, 8k, 32k, 128k tokens). Once the conversation exceeds this limit, the oldest parts of the conversation are \"forgotten\" as they fall out of the window.\\n    *   **Computational Cost:** Processing a larger context window requires more computational resources.\\n*   **Examples:** Remembering the topic of the last few sentences, referring back to something just mentioned, resolving pronouns (e.g., \"it,\" \"that\").\\n\\n### 2. Long-Term Conversational Memory\\n\\nThis type of memory allows the AI to recall information beyond the immediate context window, across multiple sessions, or for specific facts about the user or domain.\\n\\n#### a. Explicit Key-Value Store / Database Memory\\n\\n*   **Mechanism:** Storing specific pieces of information in a structured database (like a SQL database, NoSQL database, or a simple key-value store).\\n*   **How it works:** When the AI identifies a piece of information it needs to remember long-term (e.g., user\\'s name, preferences, past orders, specific facts), it stores it with a unique identifier (like a user ID). When needed, it retrieves this information.\\n*   **Use Cases:**\\n    *   **User Profiles:** Storing user preferences, demographics, account details.\\n    *   **Fact Storage:** Remembering specific facts provided by the user (e.g., \"My favorite color is blue\").\\n    *   **Session Tracking:** Storing the state of a multi-turn process (e.g., booking a flight, filling out a form).\\n*   **Advantages:** Highly reliable, easy to query for specific facts.\\n*   **Disadvantages:** Requires explicit programming to identify what to store and retrieve; not good for nuanced or semantic memory.\\n\\n#### b. Summarization / Compression Memory\\n\\n*   **Mechanism:** Instead of storing the entire conversation, the AI periodically summarizes or compresses the past dialogue into a more concise form. This summary can then be injected into the LLM\\'s context window.\\n*   **How it works:** An LLM (or a smaller model) is used to generate a summary of the conversation so far. This summary is then used as part of the prompt for subsequent turns, effectively extending the \"memory\" without exceeding the token limit.\\n*   **Use Cases:** Maintaining a high-level understanding of a long conversation, especially when details aren\\'t critical.\\n*   **Advantages:** Can extend effective memory significantly, reduces token usage.\\n*   **Disadvantages:** Information loss can occur during summarization; the summary itself consumes tokens.\\n\\n#### c. Vector Database / Embeddings Memory (Retrieval Augmented Generation - RAG)\\n\\n*   **Mechanism:** Converting past conversation turns, documents, or facts into numerical representations called \"embeddings\" and storing them in a vector database.\\n*   **How it works:** When a new user query comes in, its embedding is generated. This query embedding is then used to search the vector database for semantically similar past interactions or relevant documents. The retrieved information is then added to the LLM\\'s context window as supplementary data.\\n*   **Use Cases:**\\n    *   **Retrieving relevant past conversations:** If a user asks about a topic discussed days ago, the system can find those specific turns.\\n    *   **Knowledge Base Integration:** Allowing the AI to access and recall information from vast external knowledge bases (documents, articles, FAQs) that it wasn\\'t explicitly trained on.\\n    *   **Personalized Responses:** Retrieving past preferences or details mentioned implicitly.\\n*   **Advantages:** Highly scalable, allows for semantic retrieval (finding information based on meaning, not just keywords), powerful for grounding LLMs in specific knowledge.\\n*   **Disadvantages:** Requires a robust embedding model and vector database infrastructure; retrieval quality depends on embedding quality and search algorithms.\\n\\n### 3. Hybrid Approaches\\n\\nMost sophisticated conversational AI systems use a combination of these memory types. For example:\\n\\n*   **Short-term context window** for immediate coherence.\\n*   **Explicit database** for user profile information and critical facts.\\n*   **Vector database (RAG)** for retrieving relevant past conversations or external knowledge when needed.\\n*   **Summarization** to condense long conversations before storing them in a vector database or passing them to the LLM.\\n\\nUnderstanding these different types is key to designing AI systems that can have truly intelligent and engaging conversations! Which aspect are you most interested in exploring further, James?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--80c31b64-edab-4484-bf52-d236480f8047-0', usage_metadata={'input_tokens': 46, 'output_tokens': 1203, 'total_tokens': 1654, 'input_token_details': {'cache_read': 0}})]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"I'm researching the different types of conversational memory.\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")\n",
        "\n",
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqEv8kxZtaTS"
      },
      "source": [
        "So far so good! Let's continue with a few more messages before returning to the name question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "F7cy8dKZtaTS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n",
            "Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n",
            "Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        }
      ],
      "source": [
        "for msg in [\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]:\n",
        "    pipeline_with_history.invoke(\n",
        "        {\"query\": msg},\n",
        "        config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnOdFy4htaTS"
      },
      "source": [
        "Let's see the latest summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SEBczVutaTS",
        "outputId": "e0f91796-96fa-4daf-8f42-0557525b5bc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--0d798160-36db-4e26-8c7c-4d57b1992a16-0', usage_metadata={'input_tokens': 15, 'output_tokens': 18, 'total_tokens': 62, 'input_token_details': {'cache_read': 0}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That\\'s a fascinating and crucial area in AI and natural language processing! Conversational memory is what allows a chatbot or AI system to remember past interactions within a conversation, making the dialogue feel more natural, coherent, and personalized. Without it, every turn would be like starting a brand new conversation.\\n\\nThere are several ways to categorize and implement conversational memory, but generally, we can think of them in terms of **short-term** and **long-term** memory, often with different mechanisms for each.\\n\\nHere\\'s a breakdown of the different types:\\n\\n### 1. Short-Term Conversational Memory (Context Window)\\n\\nThis is the most common and fundamental type, primarily used to maintain coherence within a single, ongoing conversation session.\\n\\n*   **Mechanism:** For most modern large language models (LLMs), this is handled by the **context window**. The model is given a fixed number of previous turns (or tokens) from the conversation as input along with the current user query.\\n*   **How it works:** The LLM \"sees\" the recent history of the conversation and uses that information to generate a relevant and contextually appropriate response.\\n*   **Limitations:**\\n    *   **Token Limit:** LLMs have a finite context window size (e.g., 4k, 8k, 32k, 128k tokens). Once the conversation exceeds this limit, the oldest parts of the conversation are \"forgotten\" as they fall out of the window.\\n    *   **Computational Cost:** Processing a larger context window requires more computational resources.\\n*   **Examples:** Remembering the topic of the last few sentences, referring back to something just mentioned, resolving pronouns (e.g., \"it,\" \"that\").\\n\\n### 2. Long-Term Conversational Memory\\n\\nThis type of memory allows the AI to recall information beyond the immediate context window, across multiple sessions, or for specific facts about the user or domain.\\n\\n#### a. Explicit Key-Value Store / Database Memory\\n\\n*   **Mechanism:** Storing specific pieces of information in a structured database (like a SQL database, NoSQL database, or a simple key-value store).\\n*   **How it works:** When the AI identifies a piece of information it needs to remember long-term (e.g., user\\'s name, preferences, past orders, specific facts), it stores it with a unique identifier (like a user ID). When needed, it retrieves this information.\\n*   **Use Cases:**\\n    *   **User Profiles:** Storing user preferences, demographics, account details.\\n    *   **Fact Storage:** Remembering specific facts provided by the user (e.g., \"My favorite color is blue\").\\n    *   **Session Tracking:** Storing the state of a multi-turn process (e.g., booking a flight, filling out a form).\\n*   **Advantages:** Highly reliable, easy to query for specific facts.\\n*   **Disadvantages:** Requires explicit programming to identify what to store and retrieve; not good for nuanced or semantic memory.\\n\\n#### b. Summarization / Compression Memory\\n\\n*   **Mechanism:** Instead of storing the entire conversation, the AI periodically summarizes or compresses the past dialogue into a more concise form. This summary can then be injected into the LLM\\'s context window.\\n*   **How it works:** An LLM (or a smaller model) is used to generate a summary of the conversation so far. This summary is then used as part of the prompt for subsequent turns, effectively extending the \"memory\" without exceeding the token limit.\\n*   **Use Cases:** Maintaining a high-level understanding of a long conversation, especially when details aren\\'t critical.\\n*   **Advantages:** Can extend effective memory significantly, reduces token usage.\\n*   **Disadvantages:** Information loss can occur during summarization; the summary itself consumes tokens.\\n\\n#### c. Vector Database / Embeddings Memory (Retrieval Augmented Generation - RAG)\\n\\n*   **Mechanism:** Converting past conversation turns, documents, or facts into numerical representations called \"embeddings\" and storing them in a vector database.\\n*   **How it works:** When a new user query comes in, its embedding is generated. This query embedding is then used to search the vector database for semantically similar past interactions or relevant documents. The retrieved information is then added to the LLM\\'s context window as supplementary data.\\n*   **Use Cases:**\\n    *   **Retrieving relevant past conversations:** If a user asks about a topic discussed days ago, the system can find those specific turns.\\n    *   **Knowledge Base Integration:** Allowing the AI to access and recall information from vast external knowledge bases (documents, articles, FAQs) that it wasn\\'t explicitly trained on.\\n    *   **Personalized Responses:** Retrieving past preferences or details mentioned implicitly.\\n*   **Advantages:** Highly scalable, allows for semantic retrieval (finding information based on meaning, not just keywords), powerful for grounding LLMs in specific knowledge.\\n*   **Disadvantages:** Requires a robust embedding model and vector database infrastructure; retrieval quality depends on embedding quality and search algorithms.\\n\\n### 3. Hybrid Approaches\\n\\nMost sophisticated conversational AI systems use a combination of these memory types. For example:\\n\\n*   **Short-term context window** for immediate coherence.\\n*   **Explicit database** for user profile information and critical facts.\\n*   **Vector database (RAG)** for retrieving relevant past conversations or external knowledge when needed.\\n*   **Summarization** to condense long conversations before storing them in a vector database or passing them to the LLM.\\n\\nUnderstanding these different types is key to designing AI systems that can have truly intelligent and engaging conversations! Which aspect are you most interested in exploring further, James?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--80c31b64-edab-4484-bf52-d236480f8047-0', usage_metadata={'input_tokens': 46, 'output_tokens': 1203, 'total_tokens': 1654, 'input_token_details': {'cache_read': 0}}),\n",
              " HumanMessage(content='I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Ah, great! `ConversationBufferMemory` and `ConversationBufferWindowMemory` are excellent examples of **short-term conversational memory** and are very commonly used, especially in frameworks like LangChain.\\n\\nLet\\'s break them down:\\n\\n### 1. ConversationBufferMemory\\n\\n*   **Concept:** This is the simplest form of memory. It stores **all** previous messages (both user and AI) in the current conversation session.\\n*   **How it works:**\\n    *   Every time a new turn occurs, the new messages are appended to an internal buffer.\\n    *   When the AI needs to generate a response, the *entire* accumulated buffer of past messages is passed along with the current user input to the Large Language Model (LLM).\\n*   **Analogy:** Imagine a notepad where you write down every single thing said in a conversation, from start to finish.\\n*   **Advantages:**\\n    *   **Perfect Recall (within the session):** The AI has access to every detail ever mentioned in the current conversation.\\n    *   **Simplicity:** Very easy to implement and understand.\\n*   **Disadvantages:**\\n    *   **Context Window Limit:** This is its biggest drawback. LLMs have a finite \"context window\" (a maximum number of tokens they can process at once). As the conversation grows, `ConversationBufferMemory` will eventually exceed this limit, leading to:\\n        *   **Errors:** The LLM API might reject the request.\\n        *   **Truncation:** The LLM might silently truncate the input, effectively \"forgetting\" the oldest parts of the conversation anyway.\\n        *   **Increased Cost:** Processing more tokens costs more money.\\n        *   **Slower Responses:** Longer inputs take more time to process.\\n*   **Best Use Cases:** Short, focused conversations where you expect the dialogue to remain concise and not exceed the LLM\\'s context window.\\n\\n### 2. ConversationBufferWindowMemory\\n\\n*   **Concept:** This is a more practical variation designed to mitigate the context window problem. Instead of storing everything, it only stores the **last `k` interactions** (or turns) of the conversation.\\n*   **How it works:**\\n    *   You define a `k` value (e.g., `k=5`).\\n    *   As new messages come in, they are added to the buffer.\\n    *   If the buffer already contains `k` interactions, the *oldest* interaction is removed to make space for the newest one.\\n    *   When the AI needs to respond, only the messages within this `k`-sized \"window\" are passed to the LLM.\\n*   **Analogy:** Imagine a notepad that can only hold 5 pages. When you write a 6th page, the 1st page gets torn out and discarded.\\n*   **Advantages:**\\n    *   **Manages Context Window:** Prevents the conversation history from growing indefinitely, thus avoiding context window overflow.\\n    *   **More Efficient:** Reduces token usage and processing time for longer conversations compared to `ConversationBufferMemory`.\\n    *   **Maintains Recent Context:** Keeps the most recent and usually most relevant parts of the conversation in memory.\\n*   **Disadvantages:**\\n    *   **Forgets Older Information:** The AI will \"forget\" anything that falls outside the `k` window. If crucial information was mentioned early in a long conversation, it might be lost.\\n    *   **Choosing `k`:** Selecting the right `k` value can be tricky. Too small, and the AI loses context too quickly; too large, and you still risk hitting the context window limit or incurring higher costs.\\n*   **Best Use Cases:** Most general-purpose conversational AI applications where conversations can be of varying lengths, and you need a balance between memory and efficiency. It\\'s a good default choice for many chatbots.\\n\\n### How they relate to LLMs:\\n\\nBoth of these memory types essentially prepare a string of past messages (e.g., \"Human: Hello. AI: Hi there! Human: How are you? AI: I\\'m doing well.\") and prepend or append it to the current user\\'s query. This combined text then forms the complete input (the \"prompt\") that is sent to the LLM. The LLM then uses this entire prompt to generate its next response, taking into account the provided history.\\n\\nIn summary, `ConversationBufferMemory` offers complete recall but is prone to context window issues, while `ConversationBufferWindowMemory` offers a controlled, sliding window of recall, making it more robust for longer conversations at the cost of forgetting older details.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--27728d96-8f00-4710-ae3b-6b97baa3fa3d-0', usage_metadata={'input_tokens': 1265, 'output_tokens': 979, 'total_tokens': 2753, 'input_token_details': {'cache_read': 0}}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's absolutely correct, James!\\n\\n**`ConversationBufferMemory` stores the entire conversation history, from the very first message to the most recent one, within its internal buffer.**\\n\\nIt doesn't discard any past turns, which is why it's so simple but also why it can run into issues with the LLM's context window limit on longer conversations.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--be69132a-314d-461a-b1cf-b5c25655c84a-0', usage_metadata={'input_tokens': 2253, 'output_tokens': 76, 'total_tokens': 2371, 'input_token_details': {'cache_read': 1001}}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Precisely! You\\'ve got it exactly right.\\n\\n**`ConversationBufferWindowMemory` stores only the last `k` messages (or turns) of the conversation, and when a new message comes in, it drops the oldest message to maintain that `k`-sized window.**\\n\\nThis \"sliding window\" approach is what makes it more robust for longer conversations by preventing the memory from growing indefinitely.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--23968d2f-0243-4614-8979-b15c069128a1-0', usage_metadata={'input_tokens': 2344, 'output_tokens': 81, 'total_tokens': 2425, 'input_token_details': {'cache_read': 1997}})]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcR46iNxtaTS"
      },
      "source": [
        "The information about our name has been maintained, so let's see if this is enough for our LLM to correctly recall our name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doVtbwVytaTS",
        "outputId": "da21bede-7b6a-4b9b-f894-b5dcb1e23bad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is James!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--13279bf3-7b75-402f-b427-0a86e218ce3c-0', usage_metadata={'input_tokens': 2433, 'output_tokens': 5, 'total_tokens': 2438, 'input_token_details': {'cache_read': 1993}})"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"What is my name again?\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2A8VfZctaTS"
      },
      "source": [
        "Perfect! We've successfully implemented the `ConversationSummaryMemory` type using the `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpJM9TsltaTS"
      },
      "source": [
        "## 4. `ConversationSummaryBufferMemory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmF85VmAtaTS"
      },
      "source": [
        "Our final memory type acts as a combination of `ConversationSummaryMemory` and `ConversationBufferMemory`. It keeps the buffer for the conversation up until the previous `n` tokens, anything beyond that limit is summarized then dropped from the buffer. Producing something like:\n",
        "\n",
        "\n",
        "```\n",
        "# ~~ a summary of previous interactions\n",
        "The user named James introduced himself and the AI responded, introducing itself as an AI model called Zeta.\n",
        "James then said he was researching the different types of conversational memory and Zeta asked for some\n",
        "examples.\n",
        "# ~~ the most recent messages\n",
        "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
        "AI: That's interesting, what's the difference?\n",
        "Human: Buffer memory just stores the entire conversation\n",
        "AI: That makes sense, what about ConversationBufferWindowMemory?\n",
        "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
        "AI: Very cool!\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tPilUOHtaTT",
        "outputId": "df54e5c6-95fc-4f98-a9ce-7f9956aab907"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\micro\\AppData\\Local\\Temp\\ipykernel_1996\\569741439.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryBufferMemory(\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(\n",
        "    llm=llm,\n",
        "    max_token_limit=300,\n",
        "    return_messages=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvUBQXmBtaTT"
      },
      "source": [
        "As before, we set up the deprecated memory type using the `ConversationChain` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "HdcdtYdrtaTT"
      },
      "outputs": [],
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsKcrFxOtaTT"
      },
      "source": [
        "First invoke with a single message:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAZqJKHGtaTT",
        "outputId": "d117a546-8d2a-43c0-c144-8e1629fafa89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[]\n",
            "Human: Hi, my name is James\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Hi, my name is James',\n",
              " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hi James! It's really nice to meet you! I'm a large language model, an AI, created and trained by Google. You can just call me AI, or whatever you like, really! I don't have a personal name like humans do, but I'm here and ready to chat.\\n\\nMy purpose is to be helpful and informative, and I've been trained on a massive dataset of text and code, which allows me to understand and generate human-like language. This means I can answer questions, write different kinds of creative content, and engage in conversations like this one! I'm always learning and improving, too, based on interactions like ours.\\n\\nSo, James, how can I help you today, or what's on your mind? I'm all ears... or rather, all algorithms! 😊\", additional_kwargs={}, response_metadata={})],\n",
              " 'response': \"Hi James! It's really nice to meet you! I'm a large language model, an AI, created and trained by Google. You can just call me AI, or whatever you like, really! I don't have a personal name like humans do, but I'm here and ready to chat.\\n\\nMy purpose is to be helpful and informative, and I've been trained on a massive dataset of text and code, which allows me to understand and generate human-like language. This means I can answer questions, write different kinds of creative content, and engage in conversations like this one! I'm always learning and improving, too, based on interactions like ours.\\n\\nSo, James, how can I help you today, or what's on your mind? I'm all ears... or rather, all algorithms! 😊\"}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"Hi, my name is James\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma-CwiwztaTT"
      },
      "source": [
        "Looks good so far, let's continue with a few more messages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlsR1iaItaTT",
        "outputId": "30e93e9c-215a-48b0-f591-4d33ed0c425e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi James! It's really nice to meet you! I'm a large language model, an AI, created and trained by Google. You can just call me AI, or whatever you like, really! I don't have a personal name like humans do, but I'm here and ready to chat.\\n\\nMy purpose is to be helpful and informative, and I've been trained on a massive dataset of text and code, which allows me to understand and generate human-like language. This means I can answer questions, write different kinds of creative content, and engage in conversations like this one! I'm always learning and improving, too, based on interactions like ours.\\n\\nSo, James, how can I help you today, or what's on your mind? I'm all ears... or rather, all algorithms! 😊\", additional_kwargs={}, response_metadata={})]\n",
            "Human: I'm researching the different types of conversational memory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content='James introduces himself, and the AI introduces itself as a Google-trained large language model, explaining its purpose and capabilities. James then states he is researching different types of conversational memory. The AI finds this topic fascinating and proceeds to explain five key types: Short-Term/Contextual Memory, Long-Term/Episodic Memory, Semantic Memory (General Knowledge), Dialogue State Tracking, and User Profile/Preference Memory, detailing what each is, how AI uses it, and providing examples. The AI notes its own strengths lie in short-term and semantic memory, with other types often requiring additional architectural components.', additional_kwargs={}, response_metadata={})]\n",
            "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content='James introduces himself, and the AI introduces itself as a Google-trained large language model, explaining its purpose and capabilities. James then states he is researching different types of conversational memory. The AI finds this topic fascinating and proceeds to explain five key types: Short-Term/Contextual Memory, Long-Term/Episodic Memory, Semantic Memory (General Knowledge), Dialogue State Tracking, and User Profile/Preference Memory, detailing what each is, how AI uses it, and providing examples. The AI notes its own strengths lie in short-term and semantic memory, with other types often requiring additional architectural components. James then specifically mentions `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI identifies these as excellent examples of **Short-Term/Contextual Memory**, commonly used in frameworks like LangChain. It explains `ConversationBufferMemory` as a simple buffer that stores the *entire* conversation transcript, offering complete context but facing limitations with token limits, cost, and latency as conversations lengthen. In contrast, `ConversationBufferWindowMemory` maintains a \"window\" of only the most recent `N` interactions, which helps manage context length and focuses on recent information, though it risks losing older context. The AI clarifies that while its own internal processing functions similarly to a sophisticated `ConversationBufferMemory` within its current input window, it does not persistently store conversation across separate requests, emphasizing the necessity of external memory systems for building stateful conversational AI applications.', additional_kwargs={}, response_metadata={})]\n",
            "Human: Buffer memory just stores the entire conversation\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content='James introduces himself, and the AI introduces itself as a Google-trained large language model, explaining its purpose and capabilities. James then states he is researching different types of conversational memory. The AI finds this topic fascinating and proceeds to explain five key types: Short-Term/Contextual Memory, Long-Term/Episodic Memory, Semantic Memory (General Knowledge), Dialogue State Tracking, and User Profile/Preference Memory, detailing what each is, how AI uses it, and providing examples. The AI notes its own strengths lie in short-term and semantic memory, with other types often requiring additional architectural components. James then specifically mentions `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI identifies these as excellent examples of **Short-Term/Contextual Memory**, commonly used in frameworks like LangChain. It explains `ConversationBufferMemory` as a simple buffer that stores the *entire* conversation transcript, which James confirms. The AI elaborates that this memory works by maintaining a list of messages and appending each new turn, like a digital scroll, to provide the AI with the **complete historical context** for understanding references, maintaining continuity, and responding coherently. This makes it fantastic for shorter conversations, but it faces limitations with token limits, cost, and latency as conversations lengthen due to the growing transcript. In contrast, `ConversationBufferWindowMemory` maintains a \"window\" of only the most recent `N` interactions, which helps manage context length and focuses on recent information, though it risks losing older context. The AI clarifies that while its own internal processing functions similarly to a sophisticated `ConversationBufferMemory` within its current input window, it does not persistently store conversation across separate requests, emphasizing the necessity of external memory systems for building stateful conversational AI applications.', additional_kwargs={}, response_metadata={})]\n",
            "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "for msg in [\n",
        "    \"I'm researching the different types of conversational memory.\",\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]:\n",
        "    chain.invoke({\"input\": msg})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-Rj5SMftaTT"
      },
      "source": [
        "We can see with each new message the initial `SystemMessage` is updated with a new summary of the conversation. This initial `SystemMessage` is then followed by the most recent `AIMessage` and `HumanMessage` objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Dnn_N8taTT"
      },
      "source": [
        "### `ConversationSummaryBufferMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72St8qDjtaTT"
      },
      "source": [
        "As with the previous memory types, we will implement this memory type again using the `RunnableWithMessageHistory` class. In our implementation we will modify the buffer window to be based on the number of messages rather than number of tokens. This tweak will make our implementation more closely aligned with original buffer window.\n",
        "\n",
        "We will implement all of this via a new `ConversationSummaryBufferMessageHistory` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWOvrR4wtaTT"
      },
      "outputs": [],
      "source": [
        "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatGoogleGenerativeAI = Field(default_factory=ChatGoogleGenerativeAI)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, llm: ChatGoogleGenerativeAI, k: int):\n",
        "        super().__init__(llm=llm, k=k)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "\n",
        "        existing_summary: SystemMessage | None = None\n",
        "        old_messages: list[BaseMessage] | None = None\n",
        "        # see if we already have a summary message\n",
        "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
        "            print(\">> Found existing summary\")\n",
        "            existing_summary = self.messages.pop(0)\n",
        "        # add the new messages to the history\n",
        "        self.messages.extend(messages)\n",
        "        # check if we have too many messages\n",
        "        if len(self.messages) > self.k:\n",
        "            print(\n",
        "                f\">> Found {len(self.messages)} messages, dropping \"\n",
        "                f\"oldest {len(self.messages) - self.k} messages.\")\n",
        "            # Calculate how many messages to drop\n",
        "            num_to_drop = len(self.messages) - self.k\n",
        "            old_messages = self.messages[:num_to_drop]\n",
        "            self.messages = self.messages[num_to_drop:]\n",
        "            \n",
        "        # **THIS IS THE KEY CHANGE AREA**\n",
        "        if old_messages is None:\n",
        "            print(\">> No old messages to update summary with\")\n",
        "            # if there's an existing summary, add it back before returning\n",
        "            if existing_summary:\n",
        "                self.messages.insert(0, existing_summary)\n",
        "            return\n",
        "            \n",
        "        # construct the summary chat messages\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{old_messages}\"\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        # **FIX:** Convert messages to simple strings before formatting\n",
        "        existing_summary_str = existing_summary.content if existing_summary else \"N/A\"\n",
        "        \n",
        "        old_messages_str = \"\\n\".join([f\"{m.type}: {m.content}\" for m in old_messages])\n",
        "\n",
        "        # format the messages and invoke the LLM\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=existing_summary_str,\n",
        "                old_messages=old_messages_str\n",
        "            )\n",
        "        )\n",
        "        print(f\">> New summary: {new_summary.content}\")\n",
        "        #This is to show the messages before adding the new summary\n",
        "        print(self.messages)\n",
        "        # prepend the new summary to the history\n",
        "        self.messages.insert(0, SystemMessage(content=new_summary.content))\n",
        "\n",
        "    # Also, remember to re-define your clear method\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ITlLoeAtaTT"
      },
      "source": [
        "Redefine the `get_chat_history` function to use our new `ConversationSummaryBufferMessageHistory` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "rvWsP1GMtaTT"
      },
      "outputs": [],
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, llm: ChatGoogleGenerativeAI, k: int) -> ConversationSummaryBufferMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
        "    # return the chat history\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HbocfKdtaTT"
      },
      "source": [
        "Setup our pipeline with new configurable fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
        "])\n",
        "\n",
        "pipeline = prompt_template | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "cQ_-21_3taTU"
      },
      "outputs": [],
      "source": [
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatGoogleGenerativeAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the history\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10GMrRXutaTU"
      },
      "source": [
        "Finally, we invoke our runnable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWfJn7DKtaTU",
        "outputId": "397a9a9c-9469-44ea-d3a5-82d8f2cab3f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> No old messages to update summary with\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James, nice to meet you! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--40641c02-5acb-4737-8c6c-6d3027da8851-0', usage_metadata={'input_tokens': 7, 'output_tokens': 15, 'total_tokens': 203, 'input_token_details': {'cache_read': 0}})]"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
        ")\n",
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mODJviTtaTU",
        "outputId": "fc1fb499-ab3e-426c-fe48-108da13d14e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "Message 1\n",
            "---\n",
            "\n",
            ">> No old messages to update summary with\n",
            "---\n",
            "Message 2\n",
            "---\n",
            "\n",
            ">> Found 6 messages, dropping oldest 2 messages.\n",
            ">> New summary: James introduced himself, and the AI greeted him and offered assistance.\n",
            "[HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content='That\\'s a fascinating area of study, James! Conversational memory is a complex and crucial aspect of human interaction. It\\'s not just about remembering *what* was said, but also *who* said it, *when*, *why*, and the broader context and implications.\\n\\nWhile there isn\\'t one universally agreed-upon, rigid classification system, we can break down conversational memory into several key types based on their function, duration, and the kind of information they store. Here are some of the most important distinctions:\\n\\n### Primary Types of Conversational Memory\\n\\n1.  **Working Conversational Memory (Short-Term/Immediate):**\\n    *   **What it is:** This is your \"active\" memory during a conversation. It holds the most recent utterances, the current topic, the immediate goal of the interaction, and the information needed to formulate your next response. It\\'s highly transient and constantly being updated.\\n    *   **Function:** Enables coherence, turn-taking, understanding the immediate meaning of what\\'s being said, and planning your next contribution. Without it, conversations would be disjointed and impossible to follow.\\n    *   **Example:** Remembering the last sentence your interlocutor just finished so you can respond directly to it, or keeping track of the specific question they just asked.\\n\\n2.  **Episodic Conversational Memory (Long-Term/Specific Events):**\\n    *   **What it is:** The memory of specific past conversations as distinct events. This includes details like who was involved, what was discussed, when and where it took place, and often the emotional tone or significance of the conversation.\\n    *   **Function:** Builds rapport, allows for referencing shared history (\"Remember when we talked about your trip to Italy last month?\"), provides context for ongoing discussions, and contributes to the narrative of a relationship.\\n    *   **Example:** Recalling the specific conversation you had with a friend last week about their new job, including their excitement and the details they shared.\\n\\n3.  **Semantic Conversational Memory (Long-Term/General Knowledge):**\\n    *   **What it is:** This refers to your general knowledge *about* the person you\\'re conversing with, the topics you discuss, and the language itself, independent of a specific conversational event. It includes facts, concepts, and vocabulary.\\n    *   **Function:** Provides background knowledge necessary to understand and contribute to conversations. It allows you to make inferences, understand nuances, and choose appropriate language.\\n    *   **Example:** Knowing that your friend is a vegetarian (a fact learned through conversations, but not tied to one specific instance), understanding the meaning of a technical term used in a discussion, or knowing their general political leanings.\\n\\n4.  **Autobiographical Conversational Memory (Long-Term/Personal History):**\\n    *   **What it is:** A blend of episodic and semantic memory, specifically focused on the personal history and relationship with the interlocutor as built through conversations. It\\'s the cumulative story of your interactions, shaping your understanding of their personality, preferences, and recurring themes in your discussions.\\n    *   **Function:** Deepens relationships, allows for understanding motivations, predicting behavior, and tailoring communication to the individual. It\\'s crucial for maintaining long-term social bonds.\\n    *   **Example:** Remembering that a particular colleague tends to be very detail-oriented in their explanations, or that a family member often brings up stories from their childhood.\\n\\n5.  **Pragmatic Conversational Memory (Implicit/Procedural/Social):**\\n    *   **What it is:** This is often implicit or procedural memory related to the *rules and conventions* of conversation. It includes knowledge of turn-taking cues, politeness strategies, appropriate topics, non-verbal communication, and how to interpret indirect speech acts.\\n    *   **Function:** Ensures smooth and socially appropriate interaction. It helps you navigate the social landscape of conversation, avoid gaffes, and interpret subtle cues.\\n    *   **Example:** Knowing when it\\'s your turn to speak, understanding sarcasm based on tone, remembering to use formal language with a superior, or knowing how to politely interrupt.\\n\\n6.  **Emotional Conversational Memory (Affective):**\\n    *   **What it is:** The memory of the *feelings* or emotional states associated with a conversation. This can be the overall mood of the discussion (e.g., tense, joyful, frustrating) or specific emotional reactions to certain statements.\\n    *   **Function:** Influences future interactions and shapes your perception of the relationship. Positive emotional memories encourage further interaction, while negative ones might lead to avoidance or caution.\\n    *   **Example:** Remembering that a particular discussion felt very awkward, or recalling the warmth and laughter from a recent chat with a loved one.\\n\\n### Interconnectedness\\n\\nIt\\'s important to note that these types are not isolated. They constantly interact and influence each other. For instance, your working conversational memory relies on semantic memory for word meanings, and your episodic memory of past conversations informs your pragmatic choices in the current one.\\n\\nUnderstanding these different facets of conversational memory is key to comprehending how humans build relationships, share information, and navigate their social worlds. What specific aspects of conversational memory are you most interested in for your research, James?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--adf80277-fdc3-4280-b1bb-630e3b5f87ae-0', usage_metadata={'input_tokens': 35, 'output_tokens': 1107, 'total_tokens': 2489, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.', additional_kwargs={}, response_metadata={}), AIMessage(content='Excellent! You\\'re diving into the practical implementation of conversational memory in AI systems, particularly within frameworks like LangChain, which commonly use these memory types.\\n\\nWhile human conversational memory is complex and multi-faceted, AI systems like Large Language Models (LLMs) need a structured way to \"remember\" past interactions. This is where `ConversationBufferMemory` and `ConversationBufferWindowMemory` come in. They are mechanisms to store and retrieve the history of a conversation, which is then fed back into the LLM as part of the prompt to provide context.\\n\\nLet\\'s break them down:\\n\\n---\\n\\n### 1. `ConversationBufferMemory`\\n\\n*   **Concept:** This is the simplest form of conversational memory for an AI. It acts like a **complete transcript** of the entire conversation from start to finish.\\n*   **How it works:** Every user input and every AI response is appended to a growing buffer (a string or list of messages). When a new turn occurs, the *entire* accumulated conversation history is passed back to the LLM along with the new user input.\\n*   **Analogy to Human Memory:** This is somewhat akin to having a perfect, verbatim **episodic conversational memory** for every single word ever spoken in a specific conversation, with infinite capacity. You remember *everything* that was said, in order.\\n\\n*   **Pros:**\\n    *   **Complete Context:** The LLM always has access to the full history, ensuring it doesn\\'t \"forget\" anything that was said earlier.\\n    *   **Simplicity:** Easy to implement and understand.\\n\\n*   **Cons:**\\n    *   **Context Window Limits:** LLMs have a maximum number of tokens (words/sub-words) they can process in a single input. As the conversation grows, `ConversationBufferMemory` will quickly hit this limit, leading to errors or truncation of the history.\\n    *   **Cost:** More tokens in the prompt mean higher API costs for each LLM call.\\n    *   **Latency:** Longer prompts take more time for the LLM to process, increasing response times.\\n    *   **Irrelevance:** Older parts of a very long conversation might become irrelevant but still consume valuable context window space and cost.\\n\\n*   **Best Use Cases:**\\n    *   Short, focused conversations where every detail is crucial.\\n    *   Testing and debugging where you want to see the full interaction.\\n    *   When the expected conversation length is very short and won\\'t exceed the LLM\\'s context window.\\n\\n---\\n\\n### 2. `ConversationBufferWindowMemory`\\n\\n*   **Concept:** This memory type addresses the limitations of `ConversationBufferMemory` by only keeping a **fixed number of the most recent interactions** (or \"turns\") in the conversation history.\\n*   **How it works:** You define a `k` value (e.g., `k=3`). The memory will then only store the last `k` exchanges (user input + AI response). As new turns occur, the oldest turns \"fall out\" of the window and are forgotten.\\n*   **Analogy to Human Memory:** This is much closer to our **working conversational memory** and a very short-term **episodic memory**. We actively keep the most recent few utterances in mind to maintain coherence, but we don\\'t typically recall every single word from the beginning of a long discussion. We focus on what\\'s most immediately relevant.\\n\\n*   **Pros:**\\n    *   **Manages Context Window:** Keeps the prompt size relatively stable and within the LLM\\'s limits, preventing overflow.\\n    *   **Cost-Effective:** Fewer tokens in the prompt generally mean lower API costs.\\n    *   **Faster:** Shorter prompts lead to quicker LLM processing.\\n    *   **Focus on Recent Relevance:** Prioritizes the most recent parts of the conversation, which are often the most important for the current turn.\\n\\n*   **Cons:**\\n    *   **Loss of Older Context:** Information from earlier in the conversation that falls outside the window is permanently lost to the LLM. If the conversation needs to refer back to something from 10 turns ago, and `k=3`, the LLM won\\'t remember it.\\n    *   **Potential for Disorientation:** If the topic shifts back to something that was discussed long ago (and is now out of the window), the LLM might seem confused or ask for clarification.\\n\\n*   **Best Use Cases:**\\n    *   Longer, ongoing conversations where maintaining a full history is impractical.\\n    *   Chatbots for customer service or general assistance where recent interactions are most critical.\\n    *   When resource management (cost, speed) is a primary concern.\\n\\n---\\n\\n### Comparison Summary:\\n\\n| Feature             | `ConversationBufferMemory`                               | `ConversationBufferWindowMemory`                               |\\n| :------------------ | :------------------------------------------------------- | :------------------------------------------------------------- |\\n| **Storage**         | Stores the *entire* conversation history.                | Stores only the *last `k` interactions* (a \"window\").          |\\n| **Context Window**  | Prone to exceeding LLM context limits.                   | Manages context window effectively.                            |\\n| **Cost**            | Increases with conversation length.                     | Relatively stable, lower cost for long conversations.          |\\n| **Latency**         | Increases with conversation length.                     | Relatively stable, faster for long conversations.              |\\n| **Information Loss**| None (within LLM limits).                                | Older information outside the window is lost.                  |\\n| **Relevance Focus** | All history is equally available.                        | Focuses on the most recent and usually most relevant history.  |\\n| **Human Analogy**   | Perfect, infinite episodic recall of a single dialogue.  | Working memory + short-term episodic recall.                   |\\n\\n---\\n\\n### Connecting to Your Research on Human Conversational Memory:\\n\\nThese AI memory types are simplified models of how humans manage conversational information.\\n\\n*   **`ConversationBufferMemory`** tries to mimic the ideal of perfect recall, but it quickly runs into the \"cognitive load\" problem that humans also face (we can\\'t remember *everything*).\\n*   **`ConversationBufferWindowMemory`** is a more pragmatic approach, reflecting how humans prioritize recent information in their **working memory** to keep a conversation coherent. We implicitly \"forget\" older, less relevant details to make room for new ones, unless they are explicitly brought back into focus or are particularly salient (which would then be stored in our long-term **episodic memory**).\\n\\nFor more advanced AI conversational memory, researchers are exploring techniques like summarization (to condense older parts of the conversation), semantic search (to retrieve relevant past information based on meaning, not just recency), and hybrid approaches that combine these strategies to better mimic the richness of human memory.\\n\\nAre there specific scenarios or challenges in conversational memory that you\\'re particularly interested in exploring further, James?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--8bee4129-a224-4c9b-983b-79df20f02f26-0', usage_metadata={'input_tokens': 1158, 'output_tokens': 1469, 'total_tokens': 3822, 'input_token_details': {'cache_read': 0}})]\n",
            "---\n",
            "Message 3\n",
            "---\n",
            "\n",
            ">> Found existing summary\n",
            ">> Found 6 messages, dropping oldest 2 messages.\n",
            ">> New summary: James introduced himself and stated he is researching different types of conversational memory. The AI greeted him and provided a detailed explanation of conversational memory, defining it as a complex aspect of human interaction that involves remembering not just *what* was said, but also *who*, *when*, *why*, and the broader context.\n",
            "\n",
            "The AI then outlined six primary types of conversational memory, highlighting their interconnectedness:\n",
            "1.  **Working Conversational Memory:** Short-term memory for immediate utterances and current topic, crucial for coherence and turn-taking.\n",
            "2.  **Episodic Conversational Memory:** Long-term memory of specific past conversations as distinct events, including participants, topics, and emotional tone.\n",
            "3.  **Semantic Conversational Memory:** Long-term general knowledge about the interlocutor, topics, and language, independent of specific conversations.\n",
            "4.  **Autobiographical Conversational Memory:** A blend of episodic and semantic memory focused on the personal history and relationship with the interlocutor.\n",
            "5.  **Pragmatic Conversational Memory:** Implicit memory for conversational rules, conventions, turn-taking, politeness, and non-verbal cues.\n",
            "6.  **Emotional Conversational Memory:** Memory of the feelings or emotional states associated with a conversation.\n",
            "\n",
            "The AI concluded by asking James what specific aspects of conversational memory he is most interested in for his research.\n",
            "[HumanMessage(content='I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.', additional_kwargs={}, response_metadata={}), AIMessage(content='Excellent! You\\'re diving into the practical implementation of conversational memory in AI systems, particularly within frameworks like LangChain, which commonly use these memory types.\\n\\nWhile human conversational memory is complex and multi-faceted, AI systems like Large Language Models (LLMs) need a structured way to \"remember\" past interactions. This is where `ConversationBufferMemory` and `ConversationBufferWindowMemory` come in. They are mechanisms to store and retrieve the history of a conversation, which is then fed back into the LLM as part of the prompt to provide context.\\n\\nLet\\'s break them down:\\n\\n---\\n\\n### 1. `ConversationBufferMemory`\\n\\n*   **Concept:** This is the simplest form of conversational memory for an AI. It acts like a **complete transcript** of the entire conversation from start to finish.\\n*   **How it works:** Every user input and every AI response is appended to a growing buffer (a string or list of messages). When a new turn occurs, the *entire* accumulated conversation history is passed back to the LLM along with the new user input.\\n*   **Analogy to Human Memory:** This is somewhat akin to having a perfect, verbatim **episodic conversational memory** for every single word ever spoken in a specific conversation, with infinite capacity. You remember *everything* that was said, in order.\\n\\n*   **Pros:**\\n    *   **Complete Context:** The LLM always has access to the full history, ensuring it doesn\\'t \"forget\" anything that was said earlier.\\n    *   **Simplicity:** Easy to implement and understand.\\n\\n*   **Cons:**\\n    *   **Context Window Limits:** LLMs have a maximum number of tokens (words/sub-words) they can process in a single input. As the conversation grows, `ConversationBufferMemory` will quickly hit this limit, leading to errors or truncation of the history.\\n    *   **Cost:** More tokens in the prompt mean higher API costs for each LLM call.\\n    *   **Latency:** Longer prompts take more time for the LLM to process, increasing response times.\\n    *   **Irrelevance:** Older parts of a very long conversation might become irrelevant but still consume valuable context window space and cost.\\n\\n*   **Best Use Cases:**\\n    *   Short, focused conversations where every detail is crucial.\\n    *   Testing and debugging where you want to see the full interaction.\\n    *   When the expected conversation length is very short and won\\'t exceed the LLM\\'s context window.\\n\\n---\\n\\n### 2. `ConversationBufferWindowMemory`\\n\\n*   **Concept:** This memory type addresses the limitations of `ConversationBufferMemory` by only keeping a **fixed number of the most recent interactions** (or \"turns\") in the conversation history.\\n*   **How it works:** You define a `k` value (e.g., `k=3`). The memory will then only store the last `k` exchanges (user input + AI response). As new turns occur, the oldest turns \"fall out\" of the window and are forgotten.\\n*   **Analogy to Human Memory:** This is much closer to our **working conversational memory** and a very short-term **episodic memory**. We actively keep the most recent few utterances in mind to maintain coherence, but we don\\'t typically recall every single word from the beginning of a long discussion. We focus on what\\'s most immediately relevant.\\n\\n*   **Pros:**\\n    *   **Manages Context Window:** Keeps the prompt size relatively stable and within the LLM\\'s limits, preventing overflow.\\n    *   **Cost-Effective:** Fewer tokens in the prompt generally mean lower API costs.\\n    *   **Faster:** Shorter prompts lead to quicker LLM processing.\\n    *   **Focus on Recent Relevance:** Prioritizes the most recent parts of the conversation, which are often the most important for the current turn.\\n\\n*   **Cons:**\\n    *   **Loss of Older Context:** Information from earlier in the conversation that falls outside the window is permanently lost to the LLM. If the conversation needs to refer back to something from 10 turns ago, and `k=3`, the LLM won\\'t remember it.\\n    *   **Potential for Disorientation:** If the topic shifts back to something that was discussed long ago (and is now out of the window), the LLM might seem confused or ask for clarification.\\n\\n*   **Best Use Cases:**\\n    *   Longer, ongoing conversations where maintaining a full history is impractical.\\n    *   Chatbots for customer service or general assistance where recent interactions are most critical.\\n    *   When resource management (cost, speed) is a primary concern.\\n\\n---\\n\\n### Comparison Summary:\\n\\n| Feature             | `ConversationBufferMemory`                               | `ConversationBufferWindowMemory`                               |\\n| :------------------ | :------------------------------------------------------- | :------------------------------------------------------------- |\\n| **Storage**         | Stores the *entire* conversation history.                | Stores only the *last `k` interactions* (a \"window\").          |\\n| **Context Window**  | Prone to exceeding LLM context limits.                   | Manages context window effectively.                            |\\n| **Cost**            | Increases with conversation length.                     | Relatively stable, lower cost for long conversations.          |\\n| **Latency**         | Increases with conversation length.                     | Relatively stable, faster for long conversations.              |\\n| **Information Loss**| None (within LLM limits).                                | Older information outside the window is lost.                  |\\n| **Relevance Focus** | All history is equally available.                        | Focuses on the most recent and usually most relevant history.  |\\n| **Human Analogy**   | Perfect, infinite episodic recall of a single dialogue.  | Working memory + short-term episodic recall.                   |\\n\\n---\\n\\n### Connecting to Your Research on Human Conversational Memory:\\n\\nThese AI memory types are simplified models of how humans manage conversational information.\\n\\n*   **`ConversationBufferMemory`** tries to mimic the ideal of perfect recall, but it quickly runs into the \"cognitive load\" problem that humans also face (we can\\'t remember *everything*).\\n*   **`ConversationBufferWindowMemory`** is a more pragmatic approach, reflecting how humans prioritize recent information in their **working memory** to keep a conversation coherent. We implicitly \"forget\" older, less relevant details to make room for new ones, unless they are explicitly brought back into focus or are particularly salient (which would then be stored in our long-term **episodic memory**).\\n\\nFor more advanced AI conversational memory, researchers are exploring techniques like summarization (to condense older parts of the conversation), semantic search (to retrieve relevant past information based on meaning, not just recency), and hybrid approaches that combine these strategies to better mimic the richness of human memory.\\n\\nAre there specific scenarios or challenges in conversational memory that you\\'re particularly interested in exploring further, James?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--8bee4129-a224-4c9b-983b-79df20f02f26-0', usage_metadata={'input_tokens': 1158, 'output_tokens': 1469, 'total_tokens': 3822, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='Buffer memory just stores the entire conversation', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Yes, that's absolutely correct and the most straightforward way to describe it!\\n\\n**`ConversationBufferMemory` stores the entire conversation history, verbatim, from the very first turn to the most current one.**\\n\\nIt's like having a complete, unedited transcript of everything that has been said by both the user and the AI.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--14fb81bd-accc-4612-9266-f13616255ee4-0', usage_metadata={'input_tokens': 2626, 'output_tokens': 68, 'total_tokens': 2725, 'input_token_details': {'cache_read': 0}})]\n",
            "---\n",
            "Message 4\n",
            "---\n",
            "\n",
            ">> Found existing summary\n",
            ">> Found 6 messages, dropping oldest 2 messages.\n",
            ">> New summary: James introduced himself, stating his research focuses on different types of conversational memory. The AI greeted him and provided a comprehensive definition of conversational memory, emphasizing it involves remembering not just *what* was said, but also *who*, *when*, *why*, and the broader context.\n",
            "\n",
            "The AI then detailed six primary types of human conversational memory:\n",
            "1.  **Working Conversational Memory:** Short-term memory for immediate utterances and current topic.\n",
            "2.  **Episodic Conversational Memory:** Long-term memory of specific past conversations as distinct events.\n",
            "3.  **Semantic Conversational Memory:** Long-term general knowledge about interlocutors, topics, and language.\n",
            "4.  **Autobiographical Conversational Memory:** A blend of episodic and semantic memory focused on personal history with the interlocutor.\n",
            "5.  **Pragmatic Conversational Memory:** Implicit memory for conversational rules, conventions, and non-verbal cues.\n",
            "6.  **Emotional Conversational Memory:** Memory of feelings associated with a conversation.\n",
            "\n",
            "Following this, James specified his interest in `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI acknowledged these as practical implementations of conversational memory in AI systems, particularly within frameworks like LangChain, which provide structured ways for LLMs to \"remember\" past interactions by feeding conversation history back into the prompt.\n",
            "\n",
            "The AI then provided a detailed breakdown of each:\n",
            "\n",
            "1.  **`ConversationBufferMemory`**:\n",
            "    *   **Concept:** Acts as a complete transcript, storing the entire conversation history.\n",
            "    *   **How it works:** Every input and response is appended to a growing buffer, and the *entire* history is passed to the LLM with each new turn.\n",
            "    *   **Human Analogy:** Akin to a perfect, infinite **episodic conversational memory** for every word spoken.\n",
            "    *   **Pros:** Complete context, simplicity.\n",
            "    *   **Cons:** Quickly hits LLM context window limits, higher cost, increased latency, potential for irrelevance.\n",
            "    *   **Best Use Cases:** Short, focused conversations; testing/debugging; very short expected conversation lengths.\n",
            "\n",
            "2.  **`ConversationBufferWindowMemory`**:\n",
            "    *   **Concept:** Stores only a fixed number (`k`) of the most recent interactions.\n",
            "    *   **How it works:** As new turns occur, the oldest turns \"fall out\" of the window and are forgotten.\n",
            "    *   **Human Analogy:** Closer to **working conversational memory** and short-term **episodic memory**, focusing on immediate relevance.\n",
            "    *   **Pros:** Manages context window, cost-effective, faster, focuses on recent relevance.\n",
            "    *   **Cons:** Loss of older context, potential for disorientation if older topics are revisited.\n",
            "    *   **Best Use Cases:** Longer, ongoing conversations; chatbots where recent interactions are critical; when resource management is a concern.\n",
            "\n",
            "A comparison table highlighted their differences in storage, context window management, cost, latency, information loss, relevance focus, and human analogy.\n",
            "\n",
            "The AI concluded by connecting these AI memory types back to human memory: `ConversationBufferMemory` represents an ideal but impractical perfect recall, while `ConversationBufferWindowMemory` reflects the pragmatic approach of human **working memory** in prioritizing recent information. It also mentioned advanced AI techniques like summarization, semantic search, and hybrid approaches being explored to better mimic the richness of human memory. The AI then asked James what specific scenarios or challenges in conversational memory he is most interested in exploring further.\n",
            "[HumanMessage(content='Buffer memory just stores the entire conversation', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Yes, that's absolutely correct and the most straightforward way to describe it!\\n\\n**`ConversationBufferMemory` stores the entire conversation history, verbatim, from the very first turn to the most current one.**\\n\\nIt's like having a complete, unedited transcript of everything that has been said by both the user and the AI.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--14fb81bd-accc-4612-9266-f13616255ee4-0', usage_metadata={'input_tokens': 2626, 'output_tokens': 68, 'total_tokens': 2725, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Precisely! You\\'ve got it perfectly.\\n\\n**`ConversationBufferWindowMemory` stores only the last `k` messages (or turns/exchanges), and as new messages come in, the oldest ones fall out of the \"window\" and are discarded.**\\n\\nThis mechanism is crucial for managing the context length and computational resources in longer conversations.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--bb351308-cc51-4d02-be5a-4c35f89affad-0', usage_metadata={'input_tokens': 1856, 'output_tokens': 71, 'total_tokens': 1978, 'input_token_details': {'cache_read': 0}})]\n"
          ]
        }
      ],
      "source": [
        "for i, msg in enumerate([\n",
        "    \"I'm researching the different types of conversational memory.\",\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]):\n",
        "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
        "    pipeline_with_history.invoke(\n",
        "        {\"query\": msg},\n",
        "        config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU_Xx40HtaTU"
      },
      "source": [
        "There we go, we've successfully implemented the `ConversationSummaryBufferMemory` type using `RunnableWithMessageHistory`!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
