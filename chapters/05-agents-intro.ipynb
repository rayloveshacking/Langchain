{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkzhY6yaJvra"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/05-agents-intro.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjdh8R2hHyBe"
      },
      "source": [
        "#### LangChain Essentials Course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGyuCmfjHyBf"
      },
      "source": [
        "# LangChain Agents Intro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6pVPE5wHyBf"
      },
      "source": [
        "LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers.\n",
        "\n",
        "In this example, we will introduce LangChain's Agents, adding the ability to use tools such as search and calculators to complete tasks that normal LLMs cannot fufil. In this example we will be using Google Gemini 2.5 Flash as it's free."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz64zYRKHyBg"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ If using LangSmith, add your API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1E7bzVbHyBg",
        "outputId": "8072eae5-ec0e-4ddb-906d-58bfac3819ff"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"aurelioai-langchain-course-agents-intro-openai\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVozCSrXHyBg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgbH7TfgHyBg"
      },
      "source": [
        "## Introduction to Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZp6eo8RHyBg"
      },
      "source": [
        "Tools are a way augment our LLMs with code execution. A tool is simply a function formatted so that our agent can undertstand how to use it, and then execute it. Let's start by creating a few simple tools.\n",
        "\n",
        "We can use the `@tool` decorator to create an LLM-compatible tool from a standard python function — this function should include a few things for optimal performance:\n",
        "\n",
        "* A docstring describing what the tool does and when it should be used, this will be read by our LLM/agent and used to decide when to use the tool, and also how to use the tool.\n",
        "\n",
        "* Clear parameter names that ideally tell the LLM what each parameter is, if it isn't clear we make sure the docstring explains what the parameter is for and how to use it.\n",
        "\n",
        "* Both parameter and return type annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Raw-I6J9HyBh"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def add(x: float, y: float) -> float:\n",
        "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
        "    return x + y\n",
        "\n",
        "@tool\n",
        "def multiply(x: float, y: float) -> float:\n",
        "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
        "    return x * y\n",
        "\n",
        "@tool\n",
        "def exponentiate(x: float, y: float) -> float:\n",
        "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
        "    return x ** y\n",
        "\n",
        "@tool\n",
        "def subtract(x: float, y: float) -> float:\n",
        "    \"\"\"Subtract 'x' from 'y'.\"\"\"\n",
        "    return y - x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pts3fTiHyBh"
      },
      "source": [
        "With the `@tool` decorator our function is turned into a `StructuredTool` object, which we can see below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb5tBy0NHyBh",
        "outputId": "8e326eda-441a-4fcb-8858-db66638f505f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StructuredTool(name='add', description=\"Add 'x' and 'y'.\", args_schema=<class 'langchain_core.utils.pydantic.add'>, func=<function add at 0x000002DAD949E3E0>)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "add"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEBnnesZHyBh"
      },
      "source": [
        "We can see the tool name, description, and arg schema:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5jYtOtqHyBh",
        "outputId": "deb2c3bb-1056-4ea2-a463-73fba8df4f37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "add.name='add'\n",
            "add.description=\"Add 'x' and 'y'.\"\n"
          ]
        }
      ],
      "source": [
        "print(f\"{add.name=}\\n{add.description=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQS-TOjjHyBh",
        "outputId": "e8e798ad-0143-47ba-ca6b-3e5256703873"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'description': \"Add 'x' and 'y'.\",\n",
              " 'properties': {'x': {'title': 'X', 'type': 'number'},\n",
              "  'y': {'title': 'Y', 'type': 'number'}},\n",
              " 'required': ['x', 'y'],\n",
              " 'title': 'add',\n",
              " 'type': 'object'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "add.args_schema.model_json_schema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMqevcXOHyBi",
        "outputId": "703ac6a9-d45d-4017-db6e-59f6774d6001"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'description': \"Raise 'x' to the power of 'y'.\",\n",
              " 'properties': {'x': {'title': 'X', 'type': 'number'},\n",
              "  'y': {'title': 'Y', 'type': 'number'}},\n",
              " 'required': ['x', 'y'],\n",
              " 'title': 'exponentiate',\n",
              " 'type': 'object'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "exponentiate.args_schema.model_json_schema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rI3zmLLHyBi"
      },
      "source": [
        "When invoking the tool, a JSON string output by the LLM will be parsed into JSON and then consumed as kwargs, similar to the below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4xjfsVEHyBi",
        "outputId": "0a417ab7-c723-4dd1-d213-117cd7cfb1fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'x': 5, 'y': 2}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "llm_output_string = \"{\\\"x\\\": 5, \\\"y\\\": 2}\"  # this is the output from the LLM\n",
        "llm_output_dict = json.loads(llm_output_string)  # load as dictionary\n",
        "llm_output_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmgV2kgqHyBi"
      },
      "source": [
        "This is then passed into the tool function as `kwargs` (keyword arguments) as indicated by the `**` operator - the `**` operator is used to unpack the dictionary into keyword arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXMZrD7jHyBi",
        "outputId": "abede9f5-85ff-4683-e346-9a238fcd19e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "exponentiate.func(**llm_output_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeHgjrBAHyBi"
      },
      "source": [
        "This covers the basics of tools and how they work, let's move on to creating the agent itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOLTTES4HyBi"
      },
      "source": [
        "## Creating an Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PVl8e8rHyBi"
      },
      "source": [
        "We're going to construct a simple tool calling agent. We will use **L**ang**C**hain **E**pression **L**anguage (LCEL) to construct the agent. We will cover LCEL more in the next chapter, but for now - all we need to know is that our agent will be constructed using syntax and components like so:\n",
        "\n",
        "\n",
        "```\n",
        "agent = (\n",
        "    <input parameters, including chat history and user query>\n",
        "    | <prompt>\n",
        "    | <LLM with tools>\n",
        ")\n",
        "```\n",
        "\n",
        "We need this agent to remember previous interactions within the conversation. To do that, we will use the `ChatPromptTemplate` with a system message, a placeholder for our chat history, a placeholder for the user query, and finally a placeholder for the agent scratchpad.\n",
        "\n",
        "The agent scratchpad is where the agent will write it's _\"notes\"_ as it is working through multiple internal thought and tool-use steps to produce a final output to the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "d3bE5bipHyBi"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"you're a helpful assistant\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buOVKBS2HyBi"
      },
      "source": [
        "Next, we must define our LLM, we will use the `gemini-2.5-flash` model with a `temperature` of `0.0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JA_sSKeHyBi",
        "outputId": "111c9480-dc50-4532-bc33-15dfe346829a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\micro\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0.0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7_xvc8eHyBi"
      },
      "source": [
        "When creating an agent we need to add conversational memory to make the agent remember previous interactions. We'll be using the older `ConversationBufferMemory` class rather than the newer `RunnableWithMessageHistory` — the reason being that we will also be using the older `create_tool_calling_agent` and `AgentExecutor` method and class.\n",
        "\n",
        "In the `05` chapter we will be using the newer `RunnableWithMessageHistory` class as we'll be building a custom `AgentExecutor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvhle9X2HyBi",
        "outputId": "9e7e3528-46a1-4535-f418-c3f4b3f3b35e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\micro\\AppData\\Local\\Temp\\ipykernel_20592\\287820297.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",  # must align with MessagesPlaceholder variable_name\n",
        "    return_messages=True  # to return Message objects\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TkhrZ5OHyBi"
      },
      "source": [
        "Now we will initialize our agent. For that we need:\n",
        "\n",
        "* `llm`: as already defined\n",
        "* `tools`: to be defined (just a list of our previously defined tools)\n",
        "* `prompt`: as already defined\n",
        "* `memory`: as already defined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WOrJMHCiHyBj"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_tool_calling_agent\n",
        "\n",
        "tools = [add, subtract, multiply, exponentiate]\n",
        "\n",
        "agent = create_tool_calling_agent(\n",
        "    llm=llm, tools=tools, prompt=prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOaGuo4PHyBj"
      },
      "source": [
        "Our `agent` by itself is like one-step of our agent execution loop. So, if we call the `agent.invoke` method it will get the LLM to generate a single response and go no further, so no tools will be executed, and no next iterations will be performed.\n",
        "\n",
        "We can see this by asking a query that should trigger a tool call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqkrelxIHyBj",
        "outputId": "4c7a6f57-fb83-4f27-c6e6-6a1807b2519a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[ToolAgentAction(tool='multiply', tool_input={'y': 7.68, 'x': 10.7}, log=\"\\nInvoking: `multiply` with `{'y': 7.68, 'x': 10.7}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"y\": 7.68, \"x\": 10.7}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--35c1c073-226f-4986-8fb7-dec0e6ffb914-0', tool_calls=[{'name': 'multiply', 'args': {'y': 7.68, 'x': 10.7}, 'id': '845cd9b8-264f-41d2-9861-5834f94debbb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 222, 'output_tokens': 24, 'total_tokens': 329, 'input_token_details': {'cache_read': 0}})], tool_call_id='845cd9b8-264f-41d2-9861-5834f94debbb')]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.invoke({\n",
        "    \"input\": \"what is 10.7 multiplied by 7.68?\",\n",
        "    \"chat_history\": memory.chat_memory.messages,\n",
        "    \"intermediate_steps\": []  # agent will append it's internal steps here\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMUKAebFHyBj"
      },
      "source": [
        "Here, we can see the LLM has generated that we should use the `multiply` tool and the tool input should be `{\"x\": 10.7, \"y\": 7.68}`. However, the tool is not executed. For that to happen we need an agent execution loop, which will handle the multiple iterations of generation to tool calling to generation, etc.\n",
        "\n",
        "We use the `AgentExecutor` class to handle the execution loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "v1211mPQHyBj"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentExecutor\n",
        "\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Do note that a specific version of langchain-google-genai will be required for the code below to work. The version is 2.0.7, use `pip install langchain-google-genai==2.0.7`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYIYOo2LHyBj"
      },
      "source": [
        "Now let's try the same query with the executor, note that the `intermediate_steps` parameter that we added before is no longer needed as the executor handles it internally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnM6cRCEHyBj",
        "outputId": "a7dfc17f-e433-41c3-c107-4c71aa6ed17d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `multiply` with `{'y': 7.68, 'x': 10.7}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m82.17599999999999\u001b[0m\u001b[32;1m\u001b[1;3m10.7 multiplied by 7.68 is 82.176.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'what is 10.7 multiplied by 7.68?',\n",
              " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='10.7 multiplied by 7.68 is 82.176.', additional_kwargs={}, response_metadata={})],\n",
              " 'output': '10.7 multiplied by 7.68 is 82.176.'}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke({\n",
        "    \"input\": \"what is 10.7 multiplied by 7.68?\",\n",
        "    \"chat_history\": memory.chat_memory.messages,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DztfJ_x0HyBj"
      },
      "source": [
        "We can see that the `multiply` tool was invoked, producing the observation of `82.175999...`. After the observation was provided, we can see that the LLM then generated a final response of:\n",
        "\n",
        "```\n",
        "10.7 multiplied by 7.68 is approximately 82.18.\n",
        "```\n",
        "\n",
        "This final response was generated based on the original query and the tool output (ie the _observation_). We can also confirm that this answer is accurate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nJeYJSmHyBj",
        "outputId": "97dc72c9-aa8c-46b8-82b6-b032b478c8fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "82.17599999999999"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "10.7*7.68"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC9gxtRTHyBj"
      },
      "source": [
        "Let's test our agent with some memory and tool use. First, we tell it our name, then we will perform a few tool calls, then see if the agent can still recall our name.\n",
        "\n",
        "First, give the agent our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNnFrhYwHyBk",
        "outputId": "498f6492-8a65-4af4-b372-2952d67c3e03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mHello James! How can I help you today?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'My name is James',\n",
              " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='10.7 multiplied by 7.68 is 82.176.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Hello James! How can I help you today?\\n', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The answer is 1331.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The answer is 1331.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"I'm sorry, I don't have a memory of past conversations. Could you please tell me your name again?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Hello James! How can I help you today?', additional_kwargs={}, response_metadata={})],\n",
              " 'output': 'Hello James! How can I help you today?'}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke({\n",
        "    \"input\": \"My name is James\",\n",
        "    \"chat_history\": memory\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8Tx9CnoHyBk"
      },
      "source": [
        "Now let's try and get the agent to perform multiple tool calls within a single execution loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tQA-L3qHyBk",
        "outputId": "20dff2bc-0643-4978-d21d-30e0949cdcaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `multiply` with `{'y': 2.0, 'x': 4.0}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m8.0\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `add` with `{'y': 10.0, 'x': 9.0}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m19.0\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `subtract` with `{'y': 19.0, 'x': 8.0}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m11.0\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `exponentiate` with `{'y': 3.0, 'x': 11.0}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m1331.0\u001b[0m\u001b[32;1m\u001b[1;3mThe answer is 1331.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What is nine plus 10, minus 4 * 2, to the power of 3',\n",
              " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='10.7 multiplied by 7.68 is 82.176.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Hello James! How can I help you today?\\n', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The answer is 1331.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The answer is 1331.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"I'm sorry, I don't have a memory of past conversations. Could you please tell me your name again?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Hello James! How can I help you today?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The answer is 1331.', additional_kwargs={}, response_metadata={})],\n",
              " 'output': 'The answer is 1331.'}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke({\n",
        "    \"input\": \"What is nine plus 10, minus 4 * 2, to the power of 3\",\n",
        "    \"chat_history\": memory\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-v8pfp1HyBk"
      },
      "source": [
        "Let's confirm that the answer is accurate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN8RDoySHyBk",
        "outputId": "8cedcade-a727-438b-a86a-dc29e628d676"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-493"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "9+10-(4*2)**3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68FsbORDHyBk"
      },
      "source": [
        "Perfect, now let's see if the agent can still recall our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVJwmPxQHyBk",
        "outputId": "25093e06-fdcd-41f9-b354-84d4dec13c3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mYour name is James.\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What is my name',\n",
              " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='10.7 multiplied by 7.68 is 82.176.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Hello James! How can I help you today?\\n', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The answer is 1331.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The answer is 1331.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"I'm sorry, I don't have a memory of past conversations. Could you please tell me your name again?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Hello James! How can I help you today?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The answer is 1331.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Your name is James.\\n', additional_kwargs={}, response_metadata={})],\n",
              " 'output': 'Your name is James.\\n'}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke({\n",
        "    \"input\": \"What is my name\",\n",
        "    \"chat_history\": memory\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfuey3P3HyBk"
      },
      "source": [
        "The agent has successfully recalled our name. Let's move on to another agent example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAVfkOWMHyBk"
      },
      "source": [
        "## SerpAPI Weather Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bEP_biMHyBk"
      },
      "source": [
        "In this example, we'll be using the same agent and executor setup as before, but we'll be adding the [SerpAPI](https://serpapi.com/users/sign_in) service to allow our agent to search the web for information.\n",
        "\n",
        "To use this tool, you need an API key, with the free plan you can use up to 100 searches per month."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20iwu0SQHyBk",
        "outputId": "9f6f767c-10cd-446a-cce0-2482a3ee8d35"
      },
      "outputs": [],
      "source": [
        "os.environ[\"SERPAPI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wuBcxESHyBk"
      },
      "source": [
        "Here we will load the `serpapi` tool directly from the prebuilt tools that LangChain provides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "YSfVtDtvHyBk"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "\n",
        "toolbox = load_tools(tool_names=['serpapi'], llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMwqKF61HyBk"
      },
      "source": [
        "These custom tools can look into your IP address, find out where you are currently, then we will also use a secondary function to get the current date and time, then we will use this information to feed into the SerpAPI to find us the weather pattern in your area and at the time of the function calling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "nDdVlLaAHyBk"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "@tool\n",
        "def get_location_from_ip():\n",
        "    \"\"\"Get the geographical location based on the IP address.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(\"https://ipinfo.io/json\")\n",
        "        data = response.json()\n",
        "        if 'loc' in data:\n",
        "            latitude, longitude = data['loc'].split(',')\n",
        "            data = (\n",
        "                f\"Latitude: {latitude},\\n\"\n",
        "                f\"Longitude: {longitude},\\n\"\n",
        "                f\"City: {data.get('city', 'N/A')},\\n\"\n",
        "                f\"Country: {data.get('country', 'N/A')}\"\n",
        "            )\n",
        "            return data\n",
        "        else:\n",
        "            return \"Location could not be determined.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error occurred: {e}\"\n",
        "\n",
        "@tool\n",
        "def get_current_datetime() -> str:\n",
        "    \"\"\"Return the current date and time.\"\"\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Y9jMlLHyBl"
      },
      "source": [
        "We can create our prompt, this time we'll skip the `chat_history` part as we don't need it. However, you can add it if preferred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "LclOCUqxHyBl"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You're a helpful assistant\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ReAct Framework\n",
        "\n",
        "So basically the previous code required the llm to output in a very specific format so that the action input can be properly sent, but this removed that complexity and made it simply, action and action input.\n",
        "\n",
        "To be more specific:\n",
        "\n",
        "The Previous Method (Structured Tool Calling)\n",
        "\n",
        "The Goal: To get the LLM to act like a computer program. It was expected to generate a perfectly formatted, machine-readable JSON object.\n",
        "\n",
        "The Output Format: The LLM had to produce something like this:\n",
        "\n",
        "{\n",
        "  \"tool_name\": \"get_location_from_ip\",\n",
        "  \"arguments\": {}\n",
        "}\n",
        "\n",
        "The Point of Failure: This is a very strict contract. As you saw, the langchain-google-genai library and the AgentExecutor got out of sync. The executor was sending a response back to the model without the tool_name field, causing the Gemini API to reject it with the error function_response.name: Name cannot be empty. It's like trying to submit a form with a required field left blank—the system immediately rejects it.\n",
        "\n",
        "The New Method (ReAct Framework)\n",
        "\n",
        "The Goal: To get the LLM to reason like a human assistant that has access to tools. It's a conversation between the LLM's \"thoughts\" and the results of its actions.\n",
        "\n",
        "The Output Format: The LLM only needs to produce simple, human-readable plain text:\n",
        "\n",
        "Thought: I need to figure out where the user is. I can use a tool for that.\n",
        "Action: get_location_from_ip\n",
        "Action Input:\n",
        "\n",
        "Why It's Robust:\n",
        "Simplicity: It's much easier and more natural for a language model to generate these simple lines of text than it is to generate a perfectly structured JSON object. The risk of a formatting error is almost zero.\n",
        "Removes Complexity: The AgentExecutor's job becomes much simpler. Instead of parsing complex JSON, it just needs to look for the keywords Action: and Action Input:. This simple text parsing is far less brittle and works consistently across different model versions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp__t8sSHyBl"
      },
      "source": [
        "Now we create our full `tools` list, our `agent`, and the `agent_executor`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "a7xGa_U8HyBl"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_react_agent\n",
        "from langchain import hub\n",
        "\n",
        "# Your tools list remains exactly the same\n",
        "tools = toolbox + [get_current_datetime, get_location_from_ip]\n",
        "\n",
        "# 1. Get a prompt that is specifically designed for ReAct agents\n",
        "# This pulls a well-tested, robust prompt from the LangChain Hub.\n",
        "prompt = hub.pull(\"hwchase17/react-chat\")\n",
        "\n",
        "# 2. Create the ReAct agent\n",
        "# This helper function combines the LLM, tools, and the ReAct prompt.\n",
        "agent = create_react_agent(llm, tools, prompt)\n",
        "\n",
        "# 3. Create the AgentExecutor\n",
        "# This is the same as before, but it now runs our new ReAct agent.\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXIYT36aHyBl"
      },
      "source": [
        "For me I have to specify to the AI as I live in the UK and alot of places in the UK also exist in USA, which is why I explicitly state to use the country in the search as well as the town / city."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBAxuVhVHyBl",
        "outputId": "0d5fc18b-635e-4cc4-bce6-e749b34f55ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
            "Action: get_current_datetime\n",
            "Action Input:\u001b[0m\u001b[33;1m\u001b[1;3m2025-09-26 01:50:57\u001b[0m\u001b[32;1m\u001b[1;3mDo I need to use a tool? Yes\n",
            "Action: get_location_from_ip\n",
            "Action Input:\u001b[0m\u001b[38;5;200m\u001b[1;3mLatitude: 1.3280,\n",
            "Longitude: 103.7910,\n",
            "City: Bukit Timah,\n",
            "Country: SG\u001b[0m\u001b[32;1m\u001b[1;3mDo I need to use a tool? Yes\n",
            "Action: Search\n",
            "Action Input: weather in Bukit Timah, SG in Celsius\u001b[0m\u001b[36;1m\u001b[1;3m{'type': 'weather_result', 'temperature': '28', 'unit': 'Celsius', 'precipitation': '0%', 'humidity': '78%', 'wind': '8 km/h', 'location': 'Bukit Timah', 'date': 'Friday 1:00 AM', 'weather': 'Cloudy'}\u001b[0m\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
            "Final Answer: The current date and time is 2025-09-26 01:50:57. The weather in Bukit Timah, SG is cloudy with a temperature of 28 degrees Celsius, 0% precipitation, 78% humidity, and wind at 8 km/h.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "out = agent_executor.invoke({\n",
        "    \"input\": (\n",
        "        \"I have a few questions, what is the date and time right now? \"\n",
        "        \"How is the weather where I am? Please give me degrees in Celsius\"\n",
        "    ), \n",
        "    \"chat_history\": memory.chat_memory.messages\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "DNm-X4ybHyBl",
        "outputId": "578066be-57e5-4e36-8575-4ae84295a84e"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The current date and time is 2025-09-26 01:50:57. The weather in Bukit Timah, SG is cloudy with a temperature of 28 degrees Celsius, 0% precipitation, 78% humidity, and wind at 8 km/h."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(out[\"output\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EYV_5l3HyBl"
      },
      "source": [
        "That's the correct answer, and we even get the approximate answer in Celsius despite the tool returning the temperature in Fahrenheit.\n",
        "\n",
        "We've finished our into to LangChain Agents, in the next chapter we will be looking at how to create custom agents and executors.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
